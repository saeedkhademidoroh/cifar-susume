
ğŸ“œ  Logging experiment output:
/content/drive/MyDrive/src/cifar-susume/artifact/log/log_2025-05-26_14-26-43.txt

ğŸ¯  _load_previous_results

âš™ï¸   Piplining experiment 1/1

ğŸ¯  _run_single_pipeline_entry

ğŸ¯  load_config

ğŸ“‚  Loading configuration file:
/content/drive/MyDrive/src/cifar-susume/artifact/config/m9_res_full.json

ğŸ¯  _ensure_output_directories

ğŸ“‚  Ensuring output directories
/content/drive/MyDrive/src/cifar-susume/artifact/log
/content/drive/MyDrive/src/cifar-susume/artifact/checkpoint
/content/drive/MyDrive/src/cifar-susume/artifact/result
/content/drive/MyDrive/src/cifar-susume/artifact/model
/content/drive/MyDrive/src/cifar-susume/artifact/error

ğŸš€  Launching experiment m9_r1 with 'm9_res_full'

ğŸ¯  build_dataset
  0%|          | 0.00/170M [00:00<?, ?B/s]  0%|          | 32.8k/170M [00:00<18:15, 156kB/s]  0%|          | 65.5k/170M [00:00<18:11, 156kB/s]  0%|          | 98.3k/170M [00:00<18:18, 155kB/s]  0%|          | 229k/170M [00:00<08:21, 340kB/s]   0%|          | 459k/170M [00:01<04:38, 610kB/s]  1%|          | 918k/170M [00:01<02:29, 1.14MB/s]  1%|1         | 1.84M/170M [00:01<01:17, 2.19MB/s]  2%|2         | 3.70M/170M [00:01<00:38, 4.31MB/s]  4%|3         | 6.82M/170M [00:01<00:22, 7.43MB/s]  6%|5         | 9.76M/170M [00:02<00:17, 9.35MB/s]  7%|7         | 12.7M/170M [00:02<00:14, 10.7MB/s]  9%|9         | 15.7M/170M [00:02<00:13, 11.7MB/s] 11%|#1        | 18.8M/170M [00:02<00:12, 12.5MB/s] 13%|#2        | 21.9M/170M [00:02<00:11, 13.0MB/s] 15%|#4        | 25.0M/170M [00:03<00:10, 13.5MB/s] 16%|#6        | 28.1M/170M [00:03<00:10, 13.8MB/s] 18%|#8        | 31.3M/170M [00:03<00:09, 14.0MB/s] 20%|##        | 34.3M/170M [00:03<00:09, 14.0MB/s] 22%|##1       | 37.5M/170M [00:04<00:09, 14.2MB/s] 24%|##3       | 40.6M/170M [00:04<00:09, 14.2MB/s] 26%|##5       | 43.6M/170M [00:04<00:08, 14.1MB/s] 27%|##7       | 46.7M/170M [00:04<00:08, 14.2MB/s] 29%|##9       | 49.7M/170M [00:04<00:08, 14.1MB/s] 31%|###       | 52.7M/170M [00:05<00:08, 14.1MB/s] 33%|###2      | 55.8M/170M [00:05<00:07, 14.4MB/s] 35%|###4      | 58.9M/170M [00:05<00:07, 14.2MB/s] 36%|###6      | 62.1M/170M [00:05<00:07, 14.2MB/s] 38%|###8      | 65.1M/170M [00:06<00:07, 14.3MB/s] 40%|###9      | 68.2M/170M [00:06<00:07, 14.3MB/s] 42%|####1     | 71.3M/170M [00:06<00:06, 14.4MB/s] 44%|####3     | 74.4M/170M [00:06<00:06, 14.4MB/s] 45%|####5     | 77.4M/170M [00:06<00:06, 14.2MB/s] 47%|####7     | 80.5M/170M [00:07<00:06, 14.2MB/s] 49%|####8     | 83.5M/170M [00:07<00:06, 14.1MB/s] 51%|#####     | 86.5M/170M [00:07<00:06, 14.0MB/s] 52%|#####2    | 89.3M/170M [00:07<00:04, 16.3MB/s] 53%|#####3    | 91.2M/170M [00:07<00:05, 14.6MB/s] 54%|#####4    | 92.8M/170M [00:07<00:05, 13.4MB/s] 56%|#####5    | 95.3M/170M [00:08<00:05, 12.9MB/s] 58%|#####7    | 98.2M/170M [00:08<00:04, 16.1MB/s] 59%|#####8    | 100M/170M [00:08<00:04, 14.3MB/s]  60%|#####9    | 102M/170M [00:08<00:05, 12.9MB/s] 61%|######1   | 104M/170M [00:08<00:05, 12.8MB/s] 63%|######3   | 107M/170M [00:09<00:04, 13.3MB/s] 65%|######4   | 110M/170M [00:09<00:04, 13.4MB/s] 67%|######6   | 114M/170M [00:09<00:04, 13.6MB/s] 68%|######8   | 116M/170M [00:09<00:03, 13.6MB/s] 70%|#######   | 119M/170M [00:09<00:03, 13.7MB/s] 72%|#######1  | 123M/170M [00:10<00:03, 13.8MB/s] 74%|#######3  | 126M/170M [00:10<00:03, 13.8MB/s] 75%|#######5  | 128M/170M [00:10<00:03, 13.8MB/s] 77%|#######7  | 131M/170M [00:10<00:02, 13.8MB/s] 79%|#######8  | 134M/170M [00:10<00:02, 13.8MB/s] 80%|########  | 137M/170M [00:11<00:02, 13.7MB/s] 82%|########2 | 140M/170M [00:11<00:02, 14.0MB/s] 84%|########4 | 143M/170M [00:11<00:01, 14.1MB/s] 86%|########6 | 147M/170M [00:11<00:01, 14.3MB/s] 88%|########7 | 150M/170M [00:12<00:01, 14.4MB/s] 90%|########9 | 153M/170M [00:12<00:01, 14.5MB/s] 92%|#########1| 156M/170M [00:12<00:00, 14.6MB/s] 93%|#########3| 159M/170M [00:12<00:00, 14.6MB/s] 95%|#########5| 162M/170M [00:12<00:00, 14.6MB/s] 97%|#########6| 165M/170M [00:13<00:00, 14.2MB/s] 99%|#########8| 168M/170M [00:13<00:00, 16.7MB/s]100%|#########9| 170M/170M [00:13<00:00, 15.3MB/s]100%|##########| 170M/170M [00:13<00:00, 12.7MB/s]

ğŸ¯  build_augmentation_transform

ğŸ¯  build_normalization_transform

ğŸ¯  build_normalization_transform

ğŸ¯  build_model

Model: "functional"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)        â”ƒ Output Shape      â”ƒ    Param # â”ƒ Connected to      â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ input_layer         â”‚ (None, 32, 32, 3) â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d (Conv2D)     â”‚ (None, 32, 32,    â”‚        448 â”‚ input_layer[0][0] â”‚
â”‚                     â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalization â”‚ (None, 32, 32,    â”‚         64 â”‚ conv2d[0][0]      â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation          â”‚ (None, 32, 32,    â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚ (Activation)        â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_1 (Conv2D)   â”‚ (None, 32, 32,    â”‚      2,320 â”‚ activation[0][0]  â”‚
â”‚                     â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 32, 32,    â”‚         64 â”‚ conv2d_1[0][0]    â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_1        â”‚ (None, 32, 32,    â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚ (Activation)        â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_2 (Conv2D)   â”‚ (None, 32, 32,    â”‚      2,320 â”‚ activation_1[0][â€¦ â”‚
â”‚                     â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 32, 32,    â”‚         64 â”‚ conv2d_2[0][0]    â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ add (Add)           â”‚ (None, 32, 32,    â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚                     â”‚ 16)               â”‚            â”‚ activation[0][0]  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_2        â”‚ (None, 32, 32,    â”‚          0 â”‚ add[0][0]         â”‚
â”‚ (Activation)        â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_3 (Conv2D)   â”‚ (None, 32, 32,    â”‚      2,320 â”‚ activation_2[0][â€¦ â”‚
â”‚                     â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 32, 32,    â”‚         64 â”‚ conv2d_3[0][0]    â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_3        â”‚ (None, 32, 32,    â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚ (Activation)        â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_4 (Conv2D)   â”‚ (None, 32, 32,    â”‚      2,320 â”‚ activation_3[0][â€¦ â”‚
â”‚                     â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 32, 32,    â”‚         64 â”‚ conv2d_4[0][0]    â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ add_1 (Add)         â”‚ (None, 32, 32,    â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚                     â”‚ 16)               â”‚            â”‚ activation_2[0][â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_4        â”‚ (None, 32, 32,    â”‚          0 â”‚ add_1[0][0]       â”‚
â”‚ (Activation)        â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_5 (Conv2D)   â”‚ (None, 32, 32,    â”‚      2,320 â”‚ activation_4[0][â€¦ â”‚
â”‚                     â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 32, 32,    â”‚         64 â”‚ conv2d_5[0][0]    â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_5        â”‚ (None, 32, 32,    â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚ (Activation)        â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_6 (Conv2D)   â”‚ (None, 32, 32,    â”‚      2,320 â”‚ activation_5[0][â€¦ â”‚
â”‚                     â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 32, 32,    â”‚         64 â”‚ conv2d_6[0][0]    â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ add_2 (Add)         â”‚ (None, 32, 32,    â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚                     â”‚ 16)               â”‚            â”‚ activation_4[0][â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_6        â”‚ (None, 32, 32,    â”‚          0 â”‚ add_2[0][0]       â”‚
â”‚ (Activation)        â”‚ 16)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_7 (Conv2D)   â”‚ (None, 16, 16,    â”‚      4,640 â”‚ activation_6[0][â€¦ â”‚
â”‚                     â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 16, 16,    â”‚        128 â”‚ conv2d_7[0][0]    â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_7        â”‚ (None, 16, 16,    â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚ (Activation)        â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_8 (Conv2D)   â”‚ (None, 16, 16,    â”‚      9,248 â”‚ activation_7[0][â€¦ â”‚
â”‚                     â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 16, 16,    â”‚        128 â”‚ conv2d_8[0][0]    â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_9 (Conv2D)   â”‚ (None, 16, 16,    â”‚        544 â”‚ activation_6[0][â€¦ â”‚
â”‚                     â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ add_3 (Add)         â”‚ (None, 16, 16,    â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚                     â”‚ 32)               â”‚            â”‚ conv2d_9[0][0]    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_8        â”‚ (None, 16, 16,    â”‚          0 â”‚ add_3[0][0]       â”‚
â”‚ (Activation)        â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_10 (Conv2D)  â”‚ (None, 16, 16,    â”‚      9,248 â”‚ activation_8[0][â€¦ â”‚
â”‚                     â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 16, 16,    â”‚        128 â”‚ conv2d_10[0][0]   â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_9        â”‚ (None, 16, 16,    â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚ (Activation)        â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_11 (Conv2D)  â”‚ (None, 16, 16,    â”‚      9,248 â”‚ activation_9[0][â€¦ â”‚
â”‚                     â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 16, 16,    â”‚        128 â”‚ conv2d_11[0][0]   â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ add_4 (Add)         â”‚ (None, 16, 16,    â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚                     â”‚ 32)               â”‚            â”‚ activation_8[0][â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_10       â”‚ (None, 16, 16,    â”‚          0 â”‚ add_4[0][0]       â”‚
â”‚ (Activation)        â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_12 (Conv2D)  â”‚ (None, 16, 16,    â”‚      9,248 â”‚ activation_10[0]â€¦ â”‚
â”‚                     â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 16, 16,    â”‚        128 â”‚ conv2d_12[0][0]   â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_11       â”‚ (None, 16, 16,    â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚ (Activation)        â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_13 (Conv2D)  â”‚ (None, 16, 16,    â”‚      9,248 â”‚ activation_11[0]â€¦ â”‚
â”‚                     â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 16, 16,    â”‚        128 â”‚ conv2d_13[0][0]   â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ add_5 (Add)         â”‚ (None, 16, 16,    â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚                     â”‚ 32)               â”‚            â”‚ activation_10[0]â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_12       â”‚ (None, 16, 16,    â”‚          0 â”‚ add_5[0][0]       â”‚
â”‚ (Activation)        â”‚ 32)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_14 (Conv2D)  â”‚ (None, 8, 8, 64)  â”‚     18,496 â”‚ activation_12[0]â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 8, 8, 64)  â”‚        256 â”‚ conv2d_14[0][0]   â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_13       â”‚ (None, 8, 8, 64)  â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚ (Activation)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_15 (Conv2D)  â”‚ (None, 8, 8, 64)  â”‚     36,928 â”‚ activation_13[0]â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 8, 8, 64)  â”‚        256 â”‚ conv2d_15[0][0]   â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_16 (Conv2D)  â”‚ (None, 8, 8, 64)  â”‚      2,112 â”‚ activation_12[0]â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ add_6 (Add)         â”‚ (None, 8, 8, 64)  â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚                     â”‚                   â”‚            â”‚ conv2d_16[0][0]   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_14       â”‚ (None, 8, 8, 64)  â”‚          0 â”‚ add_6[0][0]       â”‚
â”‚ (Activation)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_17 (Conv2D)  â”‚ (None, 8, 8, 64)  â”‚     36,928 â”‚ activation_14[0]â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 8, 8, 64)  â”‚        256 â”‚ conv2d_17[0][0]   â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_15       â”‚ (None, 8, 8, 64)  â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚ (Activation)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_18 (Conv2D)  â”‚ (None, 8, 8, 64)  â”‚     36,928 â”‚ activation_15[0]â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 8, 8, 64)  â”‚        256 â”‚ conv2d_18[0][0]   â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ add_7 (Add)         â”‚ (None, 8, 8, 64)  â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚                     â”‚                   â”‚            â”‚ activation_14[0]â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_16       â”‚ (None, 8, 8, 64)  â”‚          0 â”‚ add_7[0][0]       â”‚
â”‚ (Activation)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_19 (Conv2D)  â”‚ (None, 8, 8, 64)  â”‚     36,928 â”‚ activation_16[0]â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 8, 8, 64)  â”‚        256 â”‚ conv2d_19[0][0]   â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_17       â”‚ (None, 8, 8, 64)  â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚ (Activation)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_20 (Conv2D)  â”‚ (None, 8, 8, 64)  â”‚     36,928 â”‚ activation_17[0]â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalizatioâ€¦ â”‚ (None, 8, 8, 64)  â”‚        256 â”‚ conv2d_20[0][0]   â”‚
â”‚ (BatchNormalizatioâ€¦ â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ add_8 (Add)         â”‚ (None, 8, 8, 64)  â”‚          0 â”‚ batch_normalizatâ€¦ â”‚
â”‚                     â”‚                   â”‚            â”‚ activation_16[0]â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ activation_18       â”‚ (None, 8, 8, 64)  â”‚          0 â”‚ add_8[0][0]       â”‚
â”‚ (Activation)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ global_average_pooâ€¦ â”‚ (None, 64)        â”‚          0 â”‚ activation_18[0]â€¦ â”‚
â”‚ (GlobalAveragePoolâ€¦ â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense (Dense)       â”‚ (None, 10)        â”‚        650 â”‚ global_average_pâ€¦ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 274,442 (1.05 MB)
 Trainable params: 273,066 (1.04 MB)
 Non-trainable params: 1,376 (5.38 KB)

ğŸ¯  train_model

ğŸ¯  _resume_from_checkpoint

ğŸ¯  _load_from_checkpoint

ğŸ¯  _split_dataset

ğŸ¯  _mixup_fn

ğŸ¯  _prepare_checkpoint_callback

ğŸ¯  __init__ (RecoveryCheckpoint)


ğŸ¯  _print_training_context

ğŸ–¥ï¸   Available compute devices:
  â€¢ /device:CPU:0 (CPU)
  â€¢ /device:GPU:0 (GPU)

ğŸ§®  GPU detected: True
  â€¢ /physical_device:GPU:0

ğŸ§   Printing training configuration:
Light Mode:         OFF â€” Using reduced dataset for fast testing
Augmentation:       OFF
L2 Regularization:  ON (Î» = 0.0005)
Dropout:            OFF (rate = 0.0)
Optimizer:          SGD (lr = 0.05)
Momentum:           0.9
LR Scheduler:       ON â€” warmup for 5 epochs, decay factor 0.1
Early Stopping:     OFF
Weight Averaging:   ON â€” starting at epoch 150
Test-Time Augment:  OFF
MixUp:              ON â€” alpha = 0.2
Epochs:             164
Batch Size:         128

ğŸ“†  Epoch 1/164

ğŸ“Š  Epoch 1 â€” Loss: 1.6804 â€” Acc: 0.4400

ğŸ“ˆ  Val â€” Loss: 1.5218 â€” Acc: 0.4642

ğŸ’¾  Saved new best model â€” Val Acc: 0.4642

ğŸ“†  Epoch 2/164

ğŸ“Š  Epoch 2 â€” Loss: 1.3466 â€” Acc: 0.6158

ğŸ“ˆ  Val â€” Loss: 1.3344 â€” Acc: 0.5400

ğŸ’¾  Saved new best model â€” Val Acc: 0.5400

ğŸ“†  Epoch 3/164

ğŸ“Š  Epoch 3 â€” Loss: 1.1874 â€” Acc: 0.6954

ğŸ“ˆ  Val â€” Loss: 1.0880 â€” Acc: 0.6266

ğŸ’¾  Saved new best model â€” Val Acc: 0.6266

ğŸ“†  Epoch 4/164

ğŸ“Š  Epoch 4 â€” Loss: 1.0644 â€” Acc: 0.7475

ğŸ“ˆ  Val â€” Loss: 0.9784 â€” Acc: 0.6780

ğŸ’¾  Saved new best model â€” Val Acc: 0.6780

ğŸ“†  Epoch 5/164

ğŸ“Š  Epoch 5 â€” Loss: 0.9896 â€” Acc: 0.7813

ğŸ“ˆ  Val â€” Loss: 0.9547 â€” Acc: 0.6786

ğŸ’¾  Saved new best model â€” Val Acc: 0.6786

ğŸ“†  Epoch 6/164

ğŸ“Š  Epoch 6 â€” Loss: 0.9461 â€” Acc: 0.8093

ğŸ“ˆ  Val â€” Loss: 0.9691 â€” Acc: 0.6868

ğŸ’¾  Saved new best model â€” Val Acc: 0.6868

ğŸ“†  Epoch 7/164

ğŸ“Š  Epoch 7 â€” Loss: 0.9055 â€” Acc: 0.8283

ğŸ“ˆ  Val â€” Loss: 0.7789 â€” Acc: 0.7480

ğŸ’¾  Saved new best model â€” Val Acc: 0.7480

ğŸ“†  Epoch 8/164

ğŸ“Š  Epoch 8 â€” Loss: 0.8655 â€” Acc: 0.8458

ğŸ“ˆ  Val â€” Loss: 0.8202 â€” Acc: 0.7404

ğŸ“†  Epoch 9/164

ğŸ“Š  Epoch 9 â€” Loss: 0.8513 â€” Acc: 0.8595

ğŸ“ˆ  Val â€” Loss: 0.7946 â€” Acc: 0.7596

ğŸ’¾  Saved new best model â€” Val Acc: 0.7596

ğŸ“†  Epoch 10/164

ğŸ“Š  Epoch 10 â€” Loss: 0.8118 â€” Acc: 0.8766

ğŸ“ˆ  Val â€” Loss: 0.7170 â€” Acc: 0.7706

ğŸ’¾  Saved new best model â€” Val Acc: 0.7706

ğŸ“†  Epoch 11/164

ğŸ“Š  Epoch 11 â€” Loss: 0.7710 â€” Acc: 0.8913

ğŸ“ˆ  Val â€” Loss: 0.7704 â€” Acc: 0.7486

ğŸ“†  Epoch 12/164

ğŸ“Š  Epoch 12 â€” Loss: 0.7585 â€” Acc: 0.9022

ğŸ“ˆ  Val â€” Loss: 0.8817 â€” Acc: 0.7158

ğŸ“†  Epoch 13/164

ğŸ“Š  Epoch 13 â€” Loss: 0.7484 â€” Acc: 0.9108

ğŸ“ˆ  Val â€” Loss: 0.8802 â€” Acc: 0.7310

ğŸ“†  Epoch 14/164

ğŸ“Š  Epoch 14 â€” Loss: 0.7337 â€” Acc: 0.9215

ğŸ“ˆ  Val â€” Loss: 0.9356 â€” Acc: 0.7070

ğŸ“†  Epoch 15/164

ğŸ“Š  Epoch 15 â€” Loss: 0.7094 â€” Acc: 0.9280

ğŸ“ˆ  Val â€” Loss: 0.8736 â€” Acc: 0.7378

ğŸ“†  Epoch 16/164

ğŸ“Š  Epoch 16 â€” Loss: 0.6732 â€” Acc: 0.9374

ğŸ“ˆ  Val â€” Loss: 0.8642 â€” Acc: 0.7326

ğŸ“†  Epoch 17/164

ğŸ“Š  Epoch 17 â€” Loss: 0.6723 â€” Acc: 0.9442

ğŸ“ˆ  Val â€” Loss: 0.7775 â€” Acc: 0.7688

ğŸ“†  Epoch 18/164

ğŸ“Š  Epoch 18 â€” Loss: 0.6618 â€” Acc: 0.9497

ğŸ“ˆ  Val â€” Loss: 0.8550 â€” Acc: 0.7414

ğŸ“†  Epoch 19/164

ğŸ“Š  Epoch 19 â€” Loss: 0.6345 â€” Acc: 0.9566

ğŸ“ˆ  Val â€” Loss: 0.8421 â€” Acc: 0.7462

ğŸ“†  Epoch 20/164

ğŸ“Š  Epoch 20 â€” Loss: 0.6243 â€” Acc: 0.9600

ğŸ“ˆ  Val â€” Loss: 0.7757 â€” Acc: 0.7670

ğŸ“†  Epoch 21/164

ğŸ“Š  Epoch 21 â€” Loss: 0.6173 â€” Acc: 0.9636

ğŸ“ˆ  Val â€” Loss: 0.7719 â€” Acc: 0.7686

ğŸ“†  Epoch 22/164

ğŸ“Š  Epoch 22 â€” Loss: 0.6312 â€” Acc: 0.9679

ğŸ“ˆ  Val â€” Loss: 0.7769 â€” Acc: 0.7720

ğŸ’¾  Saved new best model â€” Val Acc: 0.7720

ğŸ“†  Epoch 23/164

ğŸ“Š  Epoch 23 â€” Loss: 0.5979 â€” Acc: 0.9701

ğŸ“ˆ  Val â€” Loss: 0.7363 â€” Acc: 0.7760

ğŸ’¾  Saved new best model â€” Val Acc: 0.7760

ğŸ“†  Epoch 24/164

ğŸ“Š  Epoch 24 â€” Loss: 0.5906 â€” Acc: 0.9751

ğŸ“ˆ  Val â€” Loss: 0.7653 â€” Acc: 0.7674

ğŸ“†  Epoch 25/164

ğŸ“Š  Epoch 25 â€” Loss: 0.5973 â€” Acc: 0.9762

ğŸ“ˆ  Val â€” Loss: 0.7375 â€” Acc: 0.7772

ğŸ’¾  Saved new best model â€” Val Acc: 0.7772

ğŸ“†  Epoch 26/164

ğŸ“Š  Epoch 26 â€” Loss: 0.5601 â€” Acc: 0.9794

ğŸ“ˆ  Val â€” Loss: 0.6999 â€” Acc: 0.7890

ğŸ’¾  Saved new best model â€” Val Acc: 0.7890

ğŸ“†  Epoch 27/164

ğŸ“Š  Epoch 27 â€” Loss: 0.6069 â€” Acc: 0.9767

ğŸ“ˆ  Val â€” Loss: 0.7900 â€” Acc: 0.7588

ğŸ“†  Epoch 28/164

ğŸ“Š  Epoch 28 â€” Loss: 0.5676 â€” Acc: 0.9803

ğŸ“ˆ  Val â€” Loss: 0.7084 â€” Acc: 0.7806

ğŸ“†  Epoch 29/164

ğŸ“Š  Epoch 29 â€” Loss: 0.5888 â€” Acc: 0.9798

ğŸ“ˆ  Val â€” Loss: 0.7641 â€” Acc: 0.7730

ğŸ“†  Epoch 30/164

ğŸ“Š  Epoch 30 â€” Loss: 0.5826 â€” Acc: 0.9826

ğŸ“ˆ  Val â€” Loss: 0.7032 â€” Acc: 0.7920

ğŸ’¾  Saved new best model â€” Val Acc: 0.7920

ğŸ“†  Epoch 31/164

ğŸ“Š  Epoch 31 â€” Loss: 0.5445 â€” Acc: 0.9845

ğŸ“ˆ  Val â€” Loss: 0.7473 â€” Acc: 0.7768

ğŸ“†  Epoch 32/164

ğŸ“Š  Epoch 32 â€” Loss: 0.5715 â€” Acc: 0.9831

ğŸ“ˆ  Val â€” Loss: 0.7762 â€” Acc: 0.7700

ğŸ“†  Epoch 33/164

ğŸ“Š  Epoch 33 â€” Loss: 0.5887 â€” Acc: 0.9825

ğŸ“ˆ  Val â€” Loss: 0.7958 â€” Acc: 0.7702

ğŸ“†  Epoch 34/164

ğŸ“Š  Epoch 34 â€” Loss: 0.5586 â€” Acc: 0.9858

ğŸ“ˆ  Val â€” Loss: 0.7684 â€” Acc: 0.7750

ğŸ“†  Epoch 35/164

ğŸ“Š  Epoch 35 â€” Loss: 0.5671 â€” Acc: 0.9871

ğŸ“ˆ  Val â€” Loss: 0.6923 â€” Acc: 0.7926

ğŸ’¾  Saved new best model â€” Val Acc: 0.7926

ğŸ“†  Epoch 36/164

ğŸ“Š  Epoch 36 â€” Loss: 0.5730 â€” Acc: 0.9862

ğŸ“ˆ  Val â€” Loss: 0.6831 â€” Acc: 0.7928

ğŸ’¾  Saved new best model â€” Val Acc: 0.7928

ğŸ“†  Epoch 37/164

ğŸ“Š  Epoch 37 â€” Loss: 0.5611 â€” Acc: 0.9893

ğŸ“ˆ  Val â€” Loss: 0.7338 â€” Acc: 0.7880

ğŸ“†  Epoch 38/164

ğŸ“Š  Epoch 38 â€” Loss: 0.5507 â€” Acc: 0.9890

ğŸ“ˆ  Val â€” Loss: 0.7510 â€” Acc: 0.7746

ğŸ“†  Epoch 39/164

ğŸ“Š  Epoch 39 â€” Loss: 0.5720 â€” Acc: 0.9884

ğŸ“ˆ  Val â€” Loss: 0.7098 â€” Acc: 0.7928

ğŸ“†  Epoch 40/164

ğŸ“Š  Epoch 40 â€” Loss: 0.5652 â€” Acc: 0.9874

ğŸ“ˆ  Val â€” Loss: 0.6861 â€” Acc: 0.7952

ğŸ’¾  Saved new best model â€” Val Acc: 0.7952

ğŸ“†  Epoch 41/164

ğŸ“Š  Epoch 41 â€” Loss: 0.5876 â€” Acc: 0.9875

ğŸ“ˆ  Val â€” Loss: 0.7459 â€” Acc: 0.7808

ğŸ“†  Epoch 42/164

ğŸ“Š  Epoch 42 â€” Loss: 0.5626 â€” Acc: 0.9893

ğŸ“ˆ  Val â€” Loss: 0.6875 â€” Acc: 0.7954

ğŸ’¾  Saved new best model â€” Val Acc: 0.7954

ğŸ“†  Epoch 43/164

ğŸ“Š  Epoch 43 â€” Loss: 0.5504 â€” Acc: 0.9904

ğŸ“ˆ  Val â€” Loss: 0.6706 â€” Acc: 0.8012

ğŸ’¾  Saved new best model â€” Val Acc: 0.8012

ğŸ“†  Epoch 44/164

ğŸ“Š  Epoch 44 â€” Loss: 0.5522 â€” Acc: 0.9902

ğŸ“ˆ  Val â€” Loss: 0.7385 â€” Acc: 0.7910

ğŸ“†  Epoch 45/164

ğŸ“Š  Epoch 45 â€” Loss: 0.5500 â€” Acc: 0.9913

ğŸ“ˆ  Val â€” Loss: 0.7256 â€” Acc: 0.7910

ğŸ“†  Epoch 46/164

ğŸ“Š  Epoch 46 â€” Loss: 0.5424 â€” Acc: 0.9901

ğŸ“ˆ  Val â€” Loss: 0.7089 â€” Acc: 0.7878

ğŸ“†  Epoch 47/164

ğŸ“Š  Epoch 47 â€” Loss: 0.5407 â€” Acc: 0.9903

ğŸ“ˆ  Val â€” Loss: 0.7477 â€” Acc: 0.7830

ğŸ“†  Epoch 48/164

ğŸ“Š  Epoch 48 â€” Loss: 0.5491 â€” Acc: 0.9908

ğŸ“ˆ  Val â€” Loss: 0.7107 â€” Acc: 0.7940

ğŸ“†  Epoch 49/164

ğŸ“Š  Epoch 49 â€” Loss: 0.5375 â€” Acc: 0.9919

ğŸ“ˆ  Val â€” Loss: 0.7066 â€” Acc: 0.7954

ğŸ“†  Epoch 50/164

ğŸ“Š  Epoch 50 â€” Loss: 0.5337 â€” Acc: 0.9918

ğŸ“ˆ  Val â€” Loss: 0.7572 â€” Acc: 0.7818

ğŸ“†  Epoch 51/164

ğŸ“Š  Epoch 51 â€” Loss: 0.5460 â€” Acc: 0.9912

ğŸ“ˆ  Val â€” Loss: 0.7343 â€” Acc: 0.7830

ğŸ“†  Epoch 52/164

ğŸ“Š  Epoch 52 â€” Loss: 0.5194 â€” Acc: 0.9925

ğŸ“ˆ  Val â€” Loss: 0.6777 â€” Acc: 0.7978

ğŸ“†  Epoch 53/164

ğŸ“Š  Epoch 53 â€” Loss: 0.5319 â€” Acc: 0.9918

ğŸ“ˆ  Val â€” Loss: 0.7380 â€” Acc: 0.7816

ğŸ“†  Epoch 54/164

ğŸ“Š  Epoch 54 â€” Loss: 0.5403 â€” Acc: 0.9921

ğŸ“ˆ  Val â€” Loss: 0.6598 â€” Acc: 0.8114

ğŸ’¾  Saved new best model â€” Val Acc: 0.8114

ğŸ“†  Epoch 55/164

ğŸ“Š  Epoch 55 â€” Loss: 0.5184 â€” Acc: 0.9922

ğŸ“ˆ  Val â€” Loss: 0.6528 â€” Acc: 0.8096

ğŸ“†  Epoch 56/164

ğŸ“Š  Epoch 56 â€” Loss: 0.5398 â€” Acc: 0.9920

ğŸ“ˆ  Val â€” Loss: 0.7088 â€” Acc: 0.7976

ğŸ“†  Epoch 57/164

ğŸ“Š  Epoch 57 â€” Loss: 0.5106 â€” Acc: 0.9932

ğŸ“ˆ  Val â€” Loss: 0.7287 â€” Acc: 0.7860

ğŸ“†  Epoch 58/164

ğŸ“Š  Epoch 58 â€” Loss: 0.5343 â€” Acc: 0.9920

ğŸ“ˆ  Val â€” Loss: 0.7001 â€” Acc: 0.8000

ğŸ“†  Epoch 59/164

ğŸ“Š  Epoch 59 â€” Loss: 0.5503 â€” Acc: 0.9925

ğŸ“ˆ  Val â€” Loss: 0.7213 â€” Acc: 0.7876

ğŸ“†  Epoch 60/164

ğŸ“Š  Epoch 60 â€” Loss: 0.5072 â€” Acc: 0.9930

ğŸ“ˆ  Val â€” Loss: 0.7027 â€” Acc: 0.7946

ğŸ“†  Epoch 61/164

ğŸ“Š  Epoch 61 â€” Loss: 0.5406 â€” Acc: 0.9912

ğŸ“ˆ  Val â€” Loss: 0.7611 â€” Acc: 0.7788

ğŸ“†  Epoch 62/164

ğŸ“Š  Epoch 62 â€” Loss: 0.5189 â€” Acc: 0.9933

ğŸ“ˆ  Val â€” Loss: 0.7105 â€” Acc: 0.7946

ğŸ“†  Epoch 63/164

ğŸ“Š  Epoch 63 â€” Loss: 0.5139 â€” Acc: 0.9928

ğŸ“ˆ  Val â€” Loss: 0.7005 â€” Acc: 0.7924

ğŸ“†  Epoch 64/164

ğŸ“Š  Epoch 64 â€” Loss: 0.5283 â€” Acc: 0.9926

ğŸ“ˆ  Val â€” Loss: 0.7426 â€” Acc: 0.7846

ğŸ“†  Epoch 65/164

ğŸ“Š  Epoch 65 â€” Loss: 0.5249 â€” Acc: 0.9930

ğŸ“ˆ  Val â€” Loss: 0.8215 â€” Acc: 0.7684

ğŸ“†  Epoch 66/164

ğŸ“Š  Epoch 66 â€” Loss: 0.5279 â€” Acc: 0.9934

ğŸ“ˆ  Val â€” Loss: 0.7040 â€” Acc: 0.7924

ğŸ“†  Epoch 67/164

ğŸ“Š  Epoch 67 â€” Loss: 0.5335 â€” Acc: 0.9933

ğŸ“ˆ  Val â€” Loss: 0.7183 â€” Acc: 0.7844

ğŸ“†  Epoch 68/164

ğŸ“Š  Epoch 68 â€” Loss: 0.5451 â€” Acc: 0.9933

ğŸ“ˆ  Val â€” Loss: 0.7095 â€” Acc: 0.7982

ğŸ“†  Epoch 69/164

ğŸ“Š  Epoch 69 â€” Loss: 0.5281 â€” Acc: 0.9926

ğŸ“ˆ  Val â€” Loss: 0.7244 â€” Acc: 0.7922

ğŸ“†  Epoch 70/164

ğŸ“Š  Epoch 70 â€” Loss: 0.5276 â€” Acc: 0.9927

ğŸ“ˆ  Val â€” Loss: 0.6952 â€” Acc: 0.7934

ğŸ“†  Epoch 71/164

ğŸ“Š  Epoch 71 â€” Loss: 0.5494 â€” Acc: 0.9926

ğŸ“ˆ  Val â€” Loss: 0.6551 â€” Acc: 0.8034

ğŸ“†  Epoch 72/164

ğŸ“Š  Epoch 72 â€” Loss: 0.5155 â€” Acc: 0.9938

ğŸ“ˆ  Val â€” Loss: 0.7233 â€” Acc: 0.7986

ğŸ“†  Epoch 73/164

ğŸ“Š  Epoch 73 â€” Loss: 0.5058 â€” Acc: 0.9938

ğŸ“ˆ  Val â€” Loss: 0.7087 â€” Acc: 0.7926

ğŸ“†  Epoch 74/164

ğŸ“Š  Epoch 74 â€” Loss: 0.5014 â€” Acc: 0.9945

ğŸ“ˆ  Val â€” Loss: 0.7272 â€” Acc: 0.7822

ğŸ“†  Epoch 75/164

ğŸ“Š  Epoch 75 â€” Loss: 0.5119 â€” Acc: 0.9934

ğŸ“ˆ  Val â€” Loss: 0.6926 â€” Acc: 0.8032

ğŸ“†  Epoch 76/164

ğŸ“Š  Epoch 76 â€” Loss: 0.5388 â€” Acc: 0.9928

ğŸ“ˆ  Val â€” Loss: 0.7061 â€” Acc: 0.8000

ğŸ“†  Epoch 77/164

ğŸ“Š  Epoch 77 â€” Loss: 0.5299 â€” Acc: 0.9923

ğŸ“ˆ  Val â€” Loss: 0.7016 â€” Acc: 0.8034

ğŸ“†  Epoch 78/164

ğŸ“Š  Epoch 78 â€” Loss: 0.5065 â€” Acc: 0.9929

ğŸ“ˆ  Val â€” Loss: 0.7027 â€” Acc: 0.7948

ğŸ“†  Epoch 79/164

ğŸ“Š  Epoch 79 â€” Loss: 0.5263 â€” Acc: 0.9938

ğŸ“ˆ  Val â€” Loss: 0.7363 â€” Acc: 0.7918

ğŸ“†  Epoch 80/164

ğŸ“Š  Epoch 80 â€” Loss: 0.5076 â€” Acc: 0.9944

ğŸ“ˆ  Val â€” Loss: 0.7188 â€” Acc: 0.7928

ğŸ“†  Epoch 81/164

ğŸ“Š  Epoch 81 â€” Loss: 0.4760 â€” Acc: 0.9942

ğŸ“ˆ  Val â€” Loss: 0.7031 â€” Acc: 0.7870

ğŸ“†  Epoch 82/164

ğŸ“Š  Epoch 82 â€” Loss: 0.5108 â€” Acc: 0.9948

ğŸ“ˆ  Val â€” Loss: 0.7225 â€” Acc: 0.7902

ğŸ“†  Epoch 83/164

ğŸ“Š  Epoch 83 â€” Loss: 0.5535 â€” Acc: 0.9930

ğŸ“ˆ  Val â€” Loss: 0.7286 â€” Acc: 0.7906

ğŸ“†  Epoch 84/164

ğŸ“Š  Epoch 84 â€” Loss: 0.5494 â€” Acc: 0.9919

ğŸ“ˆ  Val â€” Loss: 0.6965 â€” Acc: 0.8000

ğŸ“†  Epoch 85/164

ğŸ“Š  Epoch 85 â€” Loss: 0.5290 â€” Acc: 0.9934

ğŸ“ˆ  Val â€” Loss: 0.7380 â€” Acc: 0.7892

ğŸ“†  Epoch 86/164

ğŸ“Š  Epoch 86 â€” Loss: 0.5192 â€” Acc: 0.9934

ğŸ“ˆ  Val â€” Loss: 0.6908 â€” Acc: 0.7964

ğŸ“†  Epoch 87/164

ğŸ“Š  Epoch 87 â€” Loss: 0.5160 â€” Acc: 0.9940

ğŸ“ˆ  Val â€” Loss: 0.6818 â€” Acc: 0.7996

ğŸ“†  Epoch 88/164

ğŸ“Š  Epoch 88 â€” Loss: 0.5068 â€” Acc: 0.9947

ğŸ“ˆ  Val â€” Loss: 0.7068 â€” Acc: 0.7960

ğŸ“†  Epoch 89/164

ğŸ“Š  Epoch 89 â€” Loss: 0.5200 â€” Acc: 0.9936

ğŸ“ˆ  Val â€” Loss: 0.7124 â€” Acc: 0.7962

ğŸ“†  Epoch 90/164

ğŸ“Š  Epoch 90 â€” Loss: 0.5166 â€” Acc: 0.9936

ğŸ“ˆ  Val â€” Loss: 0.6503 â€” Acc: 0.8130

ğŸ’¾  Saved new best model â€” Val Acc: 0.8130

ğŸ“†  Epoch 91/164

ğŸ“Š  Epoch 91 â€” Loss: 0.5438 â€” Acc: 0.9947

ğŸ“ˆ  Val â€” Loss: 0.6875 â€” Acc: 0.8056

ğŸ“†  Epoch 92/164

ğŸ“Š  Epoch 92 â€” Loss: 0.5165 â€” Acc: 0.9951

ğŸ“ˆ  Val â€” Loss: 0.6830 â€” Acc: 0.8040

ğŸ“†  Epoch 93/164

ğŸ“Š  Epoch 93 â€” Loss: 0.5384 â€” Acc: 0.9938

ğŸ“ˆ  Val â€” Loss: 0.7515 â€” Acc: 0.7884

ğŸ“†  Epoch 94/164

ğŸ“Š  Epoch 94 â€” Loss: 0.5118 â€” Acc: 0.9942

ğŸ“ˆ  Val â€” Loss: 0.6853 â€” Acc: 0.8008

ğŸ“†  Epoch 95/164

ğŸ“Š  Epoch 95 â€” Loss: 0.5108 â€” Acc: 0.9942

ğŸ“ˆ  Val â€” Loss: 0.7146 â€” Acc: 0.7946

ğŸ“†  Epoch 96/164

ğŸ“Š  Epoch 96 â€” Loss: 0.5092 â€” Acc: 0.9944

ğŸ“ˆ  Val â€” Loss: 0.6616 â€” Acc: 0.8112

ğŸ“†  Epoch 97/164

ğŸ“Š  Epoch 97 â€” Loss: 0.5380 â€” Acc: 0.9934

ğŸ“ˆ  Val â€” Loss: 0.7333 â€” Acc: 0.7848

ğŸ“†  Epoch 98/164

ğŸ“Š  Epoch 98 â€” Loss: 0.5136 â€” Acc: 0.9948

ğŸ“ˆ  Val â€” Loss: 0.7314 â€” Acc: 0.7992

ğŸ“†  Epoch 99/164

ğŸ“Š  Epoch 99 â€” Loss: 0.5192 â€” Acc: 0.9949

ğŸ“ˆ  Val â€” Loss: 0.7568 â€” Acc: 0.7844

ğŸ“†  Epoch 100/164

ğŸ“Š  Epoch 100 â€” Loss: 0.5012 â€” Acc: 0.9955

ğŸ“ˆ  Val â€” Loss: 0.6883 â€” Acc: 0.8020

ğŸ“†  Epoch 101/164

ğŸ“Š  Epoch 101 â€” Loss: 0.5044 â€” Acc: 0.9944

ğŸ“ˆ  Val â€” Loss: 0.7123 â€” Acc: 0.7958

ğŸ“†  Epoch 102/164

ğŸ“Š  Epoch 102 â€” Loss: 0.4964 â€” Acc: 0.9944

ğŸ“ˆ  Val â€” Loss: 0.7286 â€” Acc: 0.7914

ğŸ“†  Epoch 103/164

ğŸ“Š  Epoch 103 â€” Loss: 0.5094 â€” Acc: 0.9947

ğŸ“ˆ  Val â€” Loss: 0.6947 â€” Acc: 0.7954

ğŸ“†  Epoch 104/164

ğŸ“Š  Epoch 104 â€” Loss: 0.4988 â€” Acc: 0.9952

ğŸ“ˆ  Val â€” Loss: 0.6886 â€” Acc: 0.8038

ğŸ“†  Epoch 105/164

ğŸ“Š  Epoch 105 â€” Loss: 0.5060 â€” Acc: 0.9941

ğŸ“ˆ  Val â€” Loss: 0.6950 â€” Acc: 0.8034

ğŸ“†  Epoch 106/164

ğŸ“Š  Epoch 106 â€” Loss: 0.5140 â€” Acc: 0.9939

ğŸ“ˆ  Val â€” Loss: 0.7392 â€” Acc: 0.7890

ğŸ“†  Epoch 107/164

ğŸ“Š  Epoch 107 â€” Loss: 0.4971 â€” Acc: 0.9944

ğŸ“ˆ  Val â€” Loss: 0.7431 â€” Acc: 0.7830

ğŸ“†  Epoch 108/164

ğŸ“Š  Epoch 108 â€” Loss: 0.4901 â€” Acc: 0.9955

ğŸ“ˆ  Val â€” Loss: 0.6859 â€” Acc: 0.7956

ğŸ“†  Epoch 109/164

ğŸ“Š  Epoch 109 â€” Loss: 0.4972 â€” Acc: 0.9950

ğŸ“ˆ  Val â€” Loss: 0.7054 â€” Acc: 0.7950

ğŸ“†  Epoch 110/164

ğŸ“Š  Epoch 110 â€” Loss: 0.5153 â€” Acc: 0.9938

ğŸ“ˆ  Val â€” Loss: 0.6863 â€” Acc: 0.8032

ğŸ“†  Epoch 111/164

ğŸ“Š  Epoch 111 â€” Loss: 0.5037 â€” Acc: 0.9946

ğŸ“ˆ  Val â€” Loss: 0.7104 â€” Acc: 0.8006

ğŸ“†  Epoch 112/164

ğŸ“Š  Epoch 112 â€” Loss: 0.4843 â€” Acc: 0.9953

ğŸ“ˆ  Val â€” Loss: 0.6882 â€” Acc: 0.7988

ğŸ“†  Epoch 113/164

ğŸ“Š  Epoch 113 â€” Loss: 0.4967 â€” Acc: 0.9948

ğŸ“ˆ  Val â€” Loss: 0.7149 â€” Acc: 0.7942

ğŸ“†  Epoch 114/164

ğŸ“Š  Epoch 114 â€” Loss: 0.5052 â€” Acc: 0.9942

ğŸ“ˆ  Val â€” Loss: 0.7293 â€” Acc: 0.7938

ğŸ“†  Epoch 115/164

ğŸ“Š  Epoch 115 â€” Loss: 0.5188 â€” Acc: 0.9944

ğŸ“ˆ  Val â€” Loss: 0.6927 â€” Acc: 0.8032

ğŸ“†  Epoch 116/164

ğŸ“Š  Epoch 116 â€” Loss: 0.5361 â€” Acc: 0.9937

ğŸ“ˆ  Val â€” Loss: 0.6784 â€” Acc: 0.8106

ğŸ“†  Epoch 117/164

ğŸ“Š  Epoch 117 â€” Loss: 0.5138 â€” Acc: 0.9948

ğŸ“ˆ  Val â€” Loss: 0.6962 â€” Acc: 0.7958

ğŸ“†  Epoch 118/164

ğŸ“Š  Epoch 118 â€” Loss: 0.5198 â€” Acc: 0.9948

ğŸ“ˆ  Val â€” Loss: 0.7048 â€” Acc: 0.8010

ğŸ“†  Epoch 119/164

ğŸ“Š  Epoch 119 â€” Loss: 0.5166 â€” Acc: 0.9947

ğŸ“ˆ  Val â€” Loss: 0.7180 â€” Acc: 0.7932

ğŸ“†  Epoch 120/164

ğŸ“Š  Epoch 120 â€” Loss: 0.5185 â€” Acc: 0.9951

ğŸ“ˆ  Val â€” Loss: 0.7146 â€” Acc: 0.7992

ğŸ“†  Epoch 121/164

ğŸ“Š  Epoch 121 â€” Loss: 0.5178 â€” Acc: 0.9942

ğŸ“ˆ  Val â€” Loss: 0.7085 â€” Acc: 0.8002

ğŸ“†  Epoch 122/164

ğŸ“Š  Epoch 122 â€” Loss: 0.5232 â€” Acc: 0.9941

ğŸ“ˆ  Val â€” Loss: 0.7239 â€” Acc: 0.7890

ğŸ“†  Epoch 123/164

ğŸ“Š  Epoch 123 â€” Loss: 0.4947 â€” Acc: 0.9953

ğŸ“ˆ  Val â€” Loss: 0.7055 â€” Acc: 0.7996

ğŸ“†  Epoch 124/164

ğŸ“Š  Epoch 124 â€” Loss: 0.5244 â€” Acc: 0.9939

ğŸ“ˆ  Val â€” Loss: 0.7071 â€” Acc: 0.7960

ğŸ“†  Epoch 125/164

ğŸ“Š  Epoch 125 â€” Loss: 0.4724 â€” Acc: 0.9965

ğŸ“ˆ  Val â€” Loss: 0.7485 â€” Acc: 0.7852

ğŸ“†  Epoch 126/164

ğŸ“Š  Epoch 126 â€” Loss: 0.5286 â€” Acc: 0.9936

ğŸ“ˆ  Val â€” Loss: 0.6761 â€” Acc: 0.8024

ğŸ“†  Epoch 127/164

ğŸ“Š  Epoch 127 â€” Loss: 0.5122 â€” Acc: 0.9951

ğŸ“ˆ  Val â€” Loss: 0.7398 â€” Acc: 0.7850

ğŸ“†  Epoch 128/164

ğŸ“Š  Epoch 128 â€” Loss: 0.5299 â€” Acc: 0.9940

ğŸ“ˆ  Val â€” Loss: 0.7359 â€” Acc: 0.7882

ğŸ“†  Epoch 129/164

ğŸ“Š  Epoch 129 â€” Loss: 0.5182 â€” Acc: 0.9944

ğŸ“ˆ  Val â€” Loss: 0.7120 â€” Acc: 0.7948

ğŸ“†  Epoch 130/164

ğŸ“Š  Epoch 130 â€” Loss: 0.4917 â€” Acc: 0.9943

ğŸ“ˆ  Val â€” Loss: 0.6546 â€” Acc: 0.8064

ğŸ“†  Epoch 131/164

ğŸ“Š  Epoch 131 â€” Loss: 0.5009 â€” Acc: 0.9954

ğŸ“ˆ  Val â€” Loss: 0.7111 â€” Acc: 0.7950

ğŸ“†  Epoch 132/164

ğŸ“Š  Epoch 132 â€” Loss: 0.5041 â€” Acc: 0.9945

ğŸ“ˆ  Val â€” Loss: 0.6808 â€” Acc: 0.8108

ğŸ“†  Epoch 133/164

ğŸ“Š  Epoch 133 â€” Loss: 0.5288 â€” Acc: 0.9939

ğŸ“ˆ  Val â€” Loss: 0.6608 â€” Acc: 0.8084

ğŸ“†  Epoch 134/164

ğŸ“Š  Epoch 134 â€” Loss: 0.4488 â€” Acc: 0.9959

ğŸ“ˆ  Val â€” Loss: 0.6816 â€” Acc: 0.8054

ğŸ“†  Epoch 135/164

ğŸ“Š  Epoch 135 â€” Loss: 0.5083 â€” Acc: 0.9954

ğŸ“ˆ  Val â€” Loss: 0.6758 â€” Acc: 0.8034

ğŸ“†  Epoch 136/164

ğŸ“Š  Epoch 136 â€” Loss: 0.5201 â€” Acc: 0.9944

ğŸ“ˆ  Val â€” Loss: 0.7735 â€” Acc: 0.7800

ğŸ“†  Epoch 137/164

ğŸ“Š  Epoch 137 â€” Loss: 0.5028 â€” Acc: 0.9954

ğŸ“ˆ  Val â€” Loss: 0.6881 â€” Acc: 0.8008

ğŸ“†  Epoch 138/164

ğŸ“Š  Epoch 138 â€” Loss: 0.4933 â€” Acc: 0.9952

ğŸ“ˆ  Val â€” Loss: 0.7345 â€” Acc: 0.7912

ğŸ“†  Epoch 139/164

ğŸ“Š  Epoch 139 â€” Loss: 0.5120 â€” Acc: 0.9941

ğŸ“ˆ  Val â€” Loss: 0.7020 â€” Acc: 0.7980

ğŸ“†  Epoch 140/164

ğŸ“Š  Epoch 140 â€” Loss: 0.4911 â€” Acc: 0.9955

ğŸ“ˆ  Val â€” Loss: 0.6986 â€” Acc: 0.8012

ğŸ“†  Epoch 141/164

ğŸ“Š  Epoch 141 â€” Loss: 0.5177 â€” Acc: 0.9945

ğŸ“ˆ  Val â€” Loss: 0.6911 â€” Acc: 0.8004

ğŸ“†  Epoch 142/164

ğŸ“Š  Epoch 142 â€” Loss: 0.5116 â€” Acc: 0.9952

ğŸ“ˆ  Val â€” Loss: 0.7142 â€” Acc: 0.7998

ğŸ“†  Epoch 143/164

ğŸ“Š  Epoch 143 â€” Loss: 0.5068 â€” Acc: 0.9955

ğŸ“ˆ  Val â€” Loss: 0.6895 â€” Acc: 0.8024

ğŸ“†  Epoch 144/164

ğŸ“Š  Epoch 144 â€” Loss: 0.5045 â€” Acc: 0.9948

ğŸ“ˆ  Val â€” Loss: 0.7040 â€” Acc: 0.8006

ğŸ“†  Epoch 145/164

ğŸ“Š  Epoch 145 â€” Loss: 0.4996 â€” Acc: 0.9951

ğŸ“ˆ  Val â€” Loss: 0.7069 â€” Acc: 0.7982

ğŸ“†  Epoch 146/164

ğŸ“Š  Epoch 146 â€” Loss: 0.5127 â€” Acc: 0.9948

ğŸ“ˆ  Val â€” Loss: 0.7054 â€” Acc: 0.7956

ğŸ“†  Epoch 147/164

ğŸ“Š  Epoch 147 â€” Loss: 0.5105 â€” Acc: 0.9960

ğŸ“ˆ  Val â€” Loss: 0.7036 â€” Acc: 0.8016

ğŸ“†  Epoch 148/164

ğŸ“Š  Epoch 148 â€” Loss: 0.5049 â€” Acc: 0.9948

ğŸ“ˆ  Val â€” Loss: 0.6913 â€” Acc: 0.8020

ğŸ“†  Epoch 149/164

ğŸ“Š  Epoch 149 â€” Loss: 0.5080 â€” Acc: 0.9942

ğŸ“ˆ  Val â€” Loss: 0.7019 â€” Acc: 0.7930

ğŸ“†  Epoch 150/164

ğŸ“Š  Epoch 150 â€” Loss: 0.4970 â€” Acc: 0.9962

ğŸ“ˆ  Val â€” Loss: 0.6828 â€” Acc: 0.8056

ğŸ“†  Epoch 151/164

ğŸ“Š  Epoch 151 â€” Loss: 0.5135 â€” Acc: 0.9948

ğŸ“ˆ  Val â€” Loss: 0.6826 â€” Acc: 0.8066

ğŸ“†  Epoch 152/164

ğŸ“Š  Epoch 152 â€” Loss: 0.5214 â€” Acc: 0.9940

ğŸ“ˆ  Val â€” Loss: 0.6961 â€” Acc: 0.8024

ğŸ“†  Epoch 153/164

ğŸ“Š  Epoch 153 â€” Loss: 0.4957 â€” Acc: 0.9957

ğŸ“ˆ  Val â€” Loss: 0.6791 â€” Acc: 0.8082

ğŸ“†  Epoch 154/164

ğŸ“Š  Epoch 154 â€” Loss: 0.4984 â€” Acc: 0.9956

ğŸ“ˆ  Val â€” Loss: 0.7003 â€” Acc: 0.8012

ğŸ“†  Epoch 155/164

ğŸ“Š  Epoch 155 â€” Loss: 0.5366 â€” Acc: 0.9944

ğŸ“ˆ  Val â€” Loss: 0.7109 â€” Acc: 0.7984

ğŸ“†  Epoch 156/164

ğŸ“Š  Epoch 156 â€” Loss: 0.5093 â€” Acc: 0.9952

ğŸ“ˆ  Val â€” Loss: 0.6849 â€” Acc: 0.8042

ğŸ“†  Epoch 157/164

ğŸ“Š  Epoch 157 â€” Loss: 0.4984 â€” Acc: 0.9954

ğŸ“ˆ  Val â€” Loss: 0.6880 â€” Acc: 0.8046

ğŸ“†  Epoch 158/164

ğŸ“Š  Epoch 158 â€” Loss: 0.5040 â€” Acc: 0.9955

ğŸ“ˆ  Val â€” Loss: 0.7012 â€” Acc: 0.8046

ğŸ“†  Epoch 159/164

ğŸ“Š  Epoch 159 â€” Loss: 0.5170 â€” Acc: 0.9953

ğŸ“ˆ  Val â€” Loss: 0.7406 â€” Acc: 0.7966

ğŸ“†  Epoch 160/164

ğŸ“Š  Epoch 160 â€” Loss: 0.5160 â€” Acc: 0.9951

ğŸ“ˆ  Val â€” Loss: 0.7015 â€” Acc: 0.8030

ğŸ“†  Epoch 161/164

ğŸ“Š  Epoch 161 â€” Loss: 0.5078 â€” Acc: 0.9956

ğŸ“ˆ  Val â€” Loss: 0.7038 â€” Acc: 0.7976

ğŸ“†  Epoch 162/164

ğŸ“Š  Epoch 162 â€” Loss: 0.4901 â€” Acc: 0.9955

ğŸ“ˆ  Val â€” Loss: 0.6919 â€” Acc: 0.8068

ğŸ“†  Epoch 163/164

ğŸ“Š  Epoch 163 â€” Loss: 0.4740 â€” Acc: 0.9961

ğŸ“ˆ  Val â€” Loss: 0.7157 â€” Acc: 0.7924

ğŸ“†  Epoch 164/164

ğŸ“Š  Epoch 164 â€” Loss: 0.5276 â€” Acc: 0.9957

ğŸ“ˆ  Val â€” Loss: 0.7179 â€” Acc: 0.7978

ğŸ¯  _save_training_history

âš ï¸   Failing to save history:
Object of type float32 is not JSON serializable

ğŸ¯  extract_history_metrics

ğŸ“¥  Restored best model from:
/content/drive/MyDrive/src/cifar-susume/artifact/checkpoint/m9_r1_m9_res_full/best.keras

ğŸ¯  evaluate_model

ğŸ¯  extract_history_metrics

ğŸ¯  _create_evaluation_dictionary

ğŸ“Š  Dumping experiment results:
[
  {
    "model": 9,
    "run": 1,
    "config_name": "m9_res_full",
    "date": "2025-05-26",
    "time": "15:10:35",
    "duration": "0:43:52",
    "parameters": {
      "LIGHT_MODE": false,
      "AUGMENT_MODE": {
        "enabled": false,
        "random_crop": false,
        "random_flip": false,
        "cutout": false
      },
      "L2_MODE": {
        "enabled": true,
        "lambda": 0.0005
      },
      "DROPOUT_MODE": {
        "enabled": false,
        "rate": 0.0
      },
      "OPTIMIZER": {
        "type": "sgd",
        "learning_rate": 0.05,
        "momentum": 0.9
      },
      "SCHEDULE_MODE": {
        "enabled": true,
        "warmup_epochs": 5,
        "factor": null,
        "patience": null,
        "min_lr": null
      },
      "EARLY_STOP_MODE": {
        "enabled": false,
        "patience": null,
        "restore_best_weights": null
      },
      "AVERAGE_MODE": {
        "enabled": true,
        "start_epoch": 150
      },
      "TTA_MODE": {
        "enabled": false,
        "runs": 1
      },
      "MIXUP_MODE": {
        "enabled": true,
        "alpha": 0.2
      },
      "EPOCHS_COUNT": 164,
      "BATCH_SIZE": 128
    },
    "min_train_loss": 0.44883596897125244,
    "min_train_loss_epoch": 134,
    "max_train_acc": 0.9964500069618225,
    "max_train_acc_epoch": 125,
    "min_val_loss": 0.6502788066864014,
    "min_val_loss_epoch": 90,
    "max_val_acc": 0.8130000233650208,
    "max_val_acc_epoch": 90,
    "final_test_loss": 2.7552106380462646,
    "final_test_acc": 0.7940000295639038
  }
]

âœ…   m9 run 1 with 'm9_res_full' successfully executed

ğŸ“¦   Completed 1 total experiment runs
