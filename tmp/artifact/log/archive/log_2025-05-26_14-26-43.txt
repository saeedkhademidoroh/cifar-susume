
📜  Logging experiment output:
/content/drive/MyDrive/src/cifar-susume/artifact/log/log_2025-05-26_14-26-43.txt

🎯  _load_previous_results

⚙️   Piplining experiment 1/1

🎯  _run_single_pipeline_entry

🎯  load_config

📂  Loading configuration file:
/content/drive/MyDrive/src/cifar-susume/artifact/config/m9_res_full.json

🎯  _ensure_output_directories

📂  Ensuring output directories
/content/drive/MyDrive/src/cifar-susume/artifact/log
/content/drive/MyDrive/src/cifar-susume/artifact/checkpoint
/content/drive/MyDrive/src/cifar-susume/artifact/result
/content/drive/MyDrive/src/cifar-susume/artifact/model
/content/drive/MyDrive/src/cifar-susume/artifact/error

🚀  Launching experiment m9_r1 with 'm9_res_full'

🎯  build_dataset
  0%|          | 0.00/170M [00:00<?, ?B/s]  0%|          | 32.8k/170M [00:00<18:15, 156kB/s]  0%|          | 65.5k/170M [00:00<18:11, 156kB/s]  0%|          | 98.3k/170M [00:00<18:18, 155kB/s]  0%|          | 229k/170M [00:00<08:21, 340kB/s]   0%|          | 459k/170M [00:01<04:38, 610kB/s]  1%|          | 918k/170M [00:01<02:29, 1.14MB/s]  1%|1         | 1.84M/170M [00:01<01:17, 2.19MB/s]  2%|2         | 3.70M/170M [00:01<00:38, 4.31MB/s]  4%|3         | 6.82M/170M [00:01<00:22, 7.43MB/s]  6%|5         | 9.76M/170M [00:02<00:17, 9.35MB/s]  7%|7         | 12.7M/170M [00:02<00:14, 10.7MB/s]  9%|9         | 15.7M/170M [00:02<00:13, 11.7MB/s] 11%|#1        | 18.8M/170M [00:02<00:12, 12.5MB/s] 13%|#2        | 21.9M/170M [00:02<00:11, 13.0MB/s] 15%|#4        | 25.0M/170M [00:03<00:10, 13.5MB/s] 16%|#6        | 28.1M/170M [00:03<00:10, 13.8MB/s] 18%|#8        | 31.3M/170M [00:03<00:09, 14.0MB/s] 20%|##        | 34.3M/170M [00:03<00:09, 14.0MB/s] 22%|##1       | 37.5M/170M [00:04<00:09, 14.2MB/s] 24%|##3       | 40.6M/170M [00:04<00:09, 14.2MB/s] 26%|##5       | 43.6M/170M [00:04<00:08, 14.1MB/s] 27%|##7       | 46.7M/170M [00:04<00:08, 14.2MB/s] 29%|##9       | 49.7M/170M [00:04<00:08, 14.1MB/s] 31%|###       | 52.7M/170M [00:05<00:08, 14.1MB/s] 33%|###2      | 55.8M/170M [00:05<00:07, 14.4MB/s] 35%|###4      | 58.9M/170M [00:05<00:07, 14.2MB/s] 36%|###6      | 62.1M/170M [00:05<00:07, 14.2MB/s] 38%|###8      | 65.1M/170M [00:06<00:07, 14.3MB/s] 40%|###9      | 68.2M/170M [00:06<00:07, 14.3MB/s] 42%|####1     | 71.3M/170M [00:06<00:06, 14.4MB/s] 44%|####3     | 74.4M/170M [00:06<00:06, 14.4MB/s] 45%|####5     | 77.4M/170M [00:06<00:06, 14.2MB/s] 47%|####7     | 80.5M/170M [00:07<00:06, 14.2MB/s] 49%|####8     | 83.5M/170M [00:07<00:06, 14.1MB/s] 51%|#####     | 86.5M/170M [00:07<00:06, 14.0MB/s] 52%|#####2    | 89.3M/170M [00:07<00:04, 16.3MB/s] 53%|#####3    | 91.2M/170M [00:07<00:05, 14.6MB/s] 54%|#####4    | 92.8M/170M [00:07<00:05, 13.4MB/s] 56%|#####5    | 95.3M/170M [00:08<00:05, 12.9MB/s] 58%|#####7    | 98.2M/170M [00:08<00:04, 16.1MB/s] 59%|#####8    | 100M/170M [00:08<00:04, 14.3MB/s]  60%|#####9    | 102M/170M [00:08<00:05, 12.9MB/s] 61%|######1   | 104M/170M [00:08<00:05, 12.8MB/s] 63%|######3   | 107M/170M [00:09<00:04, 13.3MB/s] 65%|######4   | 110M/170M [00:09<00:04, 13.4MB/s] 67%|######6   | 114M/170M [00:09<00:04, 13.6MB/s] 68%|######8   | 116M/170M [00:09<00:03, 13.6MB/s] 70%|#######   | 119M/170M [00:09<00:03, 13.7MB/s] 72%|#######1  | 123M/170M [00:10<00:03, 13.8MB/s] 74%|#######3  | 126M/170M [00:10<00:03, 13.8MB/s] 75%|#######5  | 128M/170M [00:10<00:03, 13.8MB/s] 77%|#######7  | 131M/170M [00:10<00:02, 13.8MB/s] 79%|#######8  | 134M/170M [00:10<00:02, 13.8MB/s] 80%|########  | 137M/170M [00:11<00:02, 13.7MB/s] 82%|########2 | 140M/170M [00:11<00:02, 14.0MB/s] 84%|########4 | 143M/170M [00:11<00:01, 14.1MB/s] 86%|########6 | 147M/170M [00:11<00:01, 14.3MB/s] 88%|########7 | 150M/170M [00:12<00:01, 14.4MB/s] 90%|########9 | 153M/170M [00:12<00:01, 14.5MB/s] 92%|#########1| 156M/170M [00:12<00:00, 14.6MB/s] 93%|#########3| 159M/170M [00:12<00:00, 14.6MB/s] 95%|#########5| 162M/170M [00:12<00:00, 14.6MB/s] 97%|#########6| 165M/170M [00:13<00:00, 14.2MB/s] 99%|#########8| 168M/170M [00:13<00:00, 16.7MB/s]100%|#########9| 170M/170M [00:13<00:00, 15.3MB/s]100%|##########| 170M/170M [00:13<00:00, 12.7MB/s]

🎯  build_augmentation_transform

🎯  build_normalization_transform

🎯  build_normalization_transform

🎯  build_model

Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input_layer         │ (None, 32, 32, 3) │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d (Conv2D)     │ (None, 32, 32,    │        448 │ input_layer[0][0] │
│                     │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalization │ (None, 32, 32,    │         64 │ conv2d[0][0]      │
│ (BatchNormalizatio… │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation          │ (None, 32, 32,    │          0 │ batch_normalizat… │
│ (Activation)        │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_1 (Conv2D)   │ (None, 32, 32,    │      2,320 │ activation[0][0]  │
│                     │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32,    │         64 │ conv2d_1[0][0]    │
│ (BatchNormalizatio… │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_1        │ (None, 32, 32,    │          0 │ batch_normalizat… │
│ (Activation)        │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_2 (Conv2D)   │ (None, 32, 32,    │      2,320 │ activation_1[0][… │
│                     │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32,    │         64 │ conv2d_2[0][0]    │
│ (BatchNormalizatio… │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add (Add)           │ (None, 32, 32,    │          0 │ batch_normalizat… │
│                     │ 16)               │            │ activation[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_2        │ (None, 32, 32,    │          0 │ add[0][0]         │
│ (Activation)        │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_3 (Conv2D)   │ (None, 32, 32,    │      2,320 │ activation_2[0][… │
│                     │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32,    │         64 │ conv2d_3[0][0]    │
│ (BatchNormalizatio… │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_3        │ (None, 32, 32,    │          0 │ batch_normalizat… │
│ (Activation)        │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_4 (Conv2D)   │ (None, 32, 32,    │      2,320 │ activation_3[0][… │
│                     │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32,    │         64 │ conv2d_4[0][0]    │
│ (BatchNormalizatio… │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_1 (Add)         │ (None, 32, 32,    │          0 │ batch_normalizat… │
│                     │ 16)               │            │ activation_2[0][… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_4        │ (None, 32, 32,    │          0 │ add_1[0][0]       │
│ (Activation)        │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_5 (Conv2D)   │ (None, 32, 32,    │      2,320 │ activation_4[0][… │
│                     │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32,    │         64 │ conv2d_5[0][0]    │
│ (BatchNormalizatio… │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_5        │ (None, 32, 32,    │          0 │ batch_normalizat… │
│ (Activation)        │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_6 (Conv2D)   │ (None, 32, 32,    │      2,320 │ activation_5[0][… │
│                     │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32,    │         64 │ conv2d_6[0][0]    │
│ (BatchNormalizatio… │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_2 (Add)         │ (None, 32, 32,    │          0 │ batch_normalizat… │
│                     │ 16)               │            │ activation_4[0][… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_6        │ (None, 32, 32,    │          0 │ add_2[0][0]       │
│ (Activation)        │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_7 (Conv2D)   │ (None, 16, 16,    │      4,640 │ activation_6[0][… │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 16, 16,    │        128 │ conv2d_7[0][0]    │
│ (BatchNormalizatio… │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_7        │ (None, 16, 16,    │          0 │ batch_normalizat… │
│ (Activation)        │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_8 (Conv2D)   │ (None, 16, 16,    │      9,248 │ activation_7[0][… │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 16, 16,    │        128 │ conv2d_8[0][0]    │
│ (BatchNormalizatio… │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_9 (Conv2D)   │ (None, 16, 16,    │        544 │ activation_6[0][… │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_3 (Add)         │ (None, 16, 16,    │          0 │ batch_normalizat… │
│                     │ 32)               │            │ conv2d_9[0][0]    │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_8        │ (None, 16, 16,    │          0 │ add_3[0][0]       │
│ (Activation)        │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_10 (Conv2D)  │ (None, 16, 16,    │      9,248 │ activation_8[0][… │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 16, 16,    │        128 │ conv2d_10[0][0]   │
│ (BatchNormalizatio… │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_9        │ (None, 16, 16,    │          0 │ batch_normalizat… │
│ (Activation)        │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_11 (Conv2D)  │ (None, 16, 16,    │      9,248 │ activation_9[0][… │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 16, 16,    │        128 │ conv2d_11[0][0]   │
│ (BatchNormalizatio… │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_4 (Add)         │ (None, 16, 16,    │          0 │ batch_normalizat… │
│                     │ 32)               │            │ activation_8[0][… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_10       │ (None, 16, 16,    │          0 │ add_4[0][0]       │
│ (Activation)        │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_12 (Conv2D)  │ (None, 16, 16,    │      9,248 │ activation_10[0]… │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 16, 16,    │        128 │ conv2d_12[0][0]   │
│ (BatchNormalizatio… │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_11       │ (None, 16, 16,    │          0 │ batch_normalizat… │
│ (Activation)        │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_13 (Conv2D)  │ (None, 16, 16,    │      9,248 │ activation_11[0]… │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 16, 16,    │        128 │ conv2d_13[0][0]   │
│ (BatchNormalizatio… │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_5 (Add)         │ (None, 16, 16,    │          0 │ batch_normalizat… │
│                     │ 32)               │            │ activation_10[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_12       │ (None, 16, 16,    │          0 │ add_5[0][0]       │
│ (Activation)        │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_14 (Conv2D)  │ (None, 8, 8, 64)  │     18,496 │ activation_12[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 8, 8, 64)  │        256 │ conv2d_14[0][0]   │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_13       │ (None, 8, 8, 64)  │          0 │ batch_normalizat… │
│ (Activation)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_15 (Conv2D)  │ (None, 8, 8, 64)  │     36,928 │ activation_13[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 8, 8, 64)  │        256 │ conv2d_15[0][0]   │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_16 (Conv2D)  │ (None, 8, 8, 64)  │      2,112 │ activation_12[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_6 (Add)         │ (None, 8, 8, 64)  │          0 │ batch_normalizat… │
│                     │                   │            │ conv2d_16[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_14       │ (None, 8, 8, 64)  │          0 │ add_6[0][0]       │
│ (Activation)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_17 (Conv2D)  │ (None, 8, 8, 64)  │     36,928 │ activation_14[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 8, 8, 64)  │        256 │ conv2d_17[0][0]   │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_15       │ (None, 8, 8, 64)  │          0 │ batch_normalizat… │
│ (Activation)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_18 (Conv2D)  │ (None, 8, 8, 64)  │     36,928 │ activation_15[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 8, 8, 64)  │        256 │ conv2d_18[0][0]   │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_7 (Add)         │ (None, 8, 8, 64)  │          0 │ batch_normalizat… │
│                     │                   │            │ activation_14[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_16       │ (None, 8, 8, 64)  │          0 │ add_7[0][0]       │
│ (Activation)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_19 (Conv2D)  │ (None, 8, 8, 64)  │     36,928 │ activation_16[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 8, 8, 64)  │        256 │ conv2d_19[0][0]   │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_17       │ (None, 8, 8, 64)  │          0 │ batch_normalizat… │
│ (Activation)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_20 (Conv2D)  │ (None, 8, 8, 64)  │     36,928 │ activation_17[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 8, 8, 64)  │        256 │ conv2d_20[0][0]   │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_8 (Add)         │ (None, 8, 8, 64)  │          0 │ batch_normalizat… │
│                     │                   │            │ activation_16[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_18       │ (None, 8, 8, 64)  │          0 │ add_8[0][0]       │
│ (Activation)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ global_average_poo… │ (None, 64)        │          0 │ activation_18[0]… │
│ (GlobalAveragePool… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense (Dense)       │ (None, 10)        │        650 │ global_average_p… │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 274,442 (1.05 MB)
 Trainable params: 273,066 (1.04 MB)
 Non-trainable params: 1,376 (5.38 KB)

🎯  train_model

🎯  _resume_from_checkpoint

🎯  _load_from_checkpoint

🎯  _split_dataset

🎯  _mixup_fn

🎯  _prepare_checkpoint_callback

🎯  __init__ (RecoveryCheckpoint)


🎯  _print_training_context

🖥️   Available compute devices:
  • /device:CPU:0 (CPU)
  • /device:GPU:0 (GPU)

🧮  GPU detected: True
  • /physical_device:GPU:0

🧠  Printing training configuration:
Light Mode:         OFF — Using reduced dataset for fast testing
Augmentation:       OFF
L2 Regularization:  ON (λ = 0.0005)
Dropout:            OFF (rate = 0.0)
Optimizer:          SGD (lr = 0.05)
Momentum:           0.9
LR Scheduler:       ON — warmup for 5 epochs, decay factor 0.1
Early Stopping:     OFF
Weight Averaging:   ON — starting at epoch 150
Test-Time Augment:  OFF
MixUp:              ON — alpha = 0.2
Epochs:             164
Batch Size:         128

📆  Epoch 1/164

📊  Epoch 1 — Loss: 1.6804 — Acc: 0.4400

📈  Val — Loss: 1.5218 — Acc: 0.4642

💾  Saved new best model — Val Acc: 0.4642

📆  Epoch 2/164

📊  Epoch 2 — Loss: 1.3466 — Acc: 0.6158

📈  Val — Loss: 1.3344 — Acc: 0.5400

💾  Saved new best model — Val Acc: 0.5400

📆  Epoch 3/164

📊  Epoch 3 — Loss: 1.1874 — Acc: 0.6954

📈  Val — Loss: 1.0880 — Acc: 0.6266

💾  Saved new best model — Val Acc: 0.6266

📆  Epoch 4/164

📊  Epoch 4 — Loss: 1.0644 — Acc: 0.7475

📈  Val — Loss: 0.9784 — Acc: 0.6780

💾  Saved new best model — Val Acc: 0.6780

📆  Epoch 5/164

📊  Epoch 5 — Loss: 0.9896 — Acc: 0.7813

📈  Val — Loss: 0.9547 — Acc: 0.6786

💾  Saved new best model — Val Acc: 0.6786

📆  Epoch 6/164

📊  Epoch 6 — Loss: 0.9461 — Acc: 0.8093

📈  Val — Loss: 0.9691 — Acc: 0.6868

💾  Saved new best model — Val Acc: 0.6868

📆  Epoch 7/164

📊  Epoch 7 — Loss: 0.9055 — Acc: 0.8283

📈  Val — Loss: 0.7789 — Acc: 0.7480

💾  Saved new best model — Val Acc: 0.7480

📆  Epoch 8/164

📊  Epoch 8 — Loss: 0.8655 — Acc: 0.8458

📈  Val — Loss: 0.8202 — Acc: 0.7404

📆  Epoch 9/164

📊  Epoch 9 — Loss: 0.8513 — Acc: 0.8595

📈  Val — Loss: 0.7946 — Acc: 0.7596

💾  Saved new best model — Val Acc: 0.7596

📆  Epoch 10/164

📊  Epoch 10 — Loss: 0.8118 — Acc: 0.8766

📈  Val — Loss: 0.7170 — Acc: 0.7706

💾  Saved new best model — Val Acc: 0.7706

📆  Epoch 11/164

📊  Epoch 11 — Loss: 0.7710 — Acc: 0.8913

📈  Val — Loss: 0.7704 — Acc: 0.7486

📆  Epoch 12/164

📊  Epoch 12 — Loss: 0.7585 — Acc: 0.9022

📈  Val — Loss: 0.8817 — Acc: 0.7158

📆  Epoch 13/164

📊  Epoch 13 — Loss: 0.7484 — Acc: 0.9108

📈  Val — Loss: 0.8802 — Acc: 0.7310

📆  Epoch 14/164

📊  Epoch 14 — Loss: 0.7337 — Acc: 0.9215

📈  Val — Loss: 0.9356 — Acc: 0.7070

📆  Epoch 15/164

📊  Epoch 15 — Loss: 0.7094 — Acc: 0.9280

📈  Val — Loss: 0.8736 — Acc: 0.7378

📆  Epoch 16/164

📊  Epoch 16 — Loss: 0.6732 — Acc: 0.9374

📈  Val — Loss: 0.8642 — Acc: 0.7326

📆  Epoch 17/164

📊  Epoch 17 — Loss: 0.6723 — Acc: 0.9442

📈  Val — Loss: 0.7775 — Acc: 0.7688

📆  Epoch 18/164

📊  Epoch 18 — Loss: 0.6618 — Acc: 0.9497

📈  Val — Loss: 0.8550 — Acc: 0.7414

📆  Epoch 19/164

📊  Epoch 19 — Loss: 0.6345 — Acc: 0.9566

📈  Val — Loss: 0.8421 — Acc: 0.7462

📆  Epoch 20/164

📊  Epoch 20 — Loss: 0.6243 — Acc: 0.9600

📈  Val — Loss: 0.7757 — Acc: 0.7670

📆  Epoch 21/164

📊  Epoch 21 — Loss: 0.6173 — Acc: 0.9636

📈  Val — Loss: 0.7719 — Acc: 0.7686

📆  Epoch 22/164

📊  Epoch 22 — Loss: 0.6312 — Acc: 0.9679

📈  Val — Loss: 0.7769 — Acc: 0.7720

💾  Saved new best model — Val Acc: 0.7720

📆  Epoch 23/164

📊  Epoch 23 — Loss: 0.5979 — Acc: 0.9701

📈  Val — Loss: 0.7363 — Acc: 0.7760

💾  Saved new best model — Val Acc: 0.7760

📆  Epoch 24/164

📊  Epoch 24 — Loss: 0.5906 — Acc: 0.9751

📈  Val — Loss: 0.7653 — Acc: 0.7674

📆  Epoch 25/164

📊  Epoch 25 — Loss: 0.5973 — Acc: 0.9762

📈  Val — Loss: 0.7375 — Acc: 0.7772

💾  Saved new best model — Val Acc: 0.7772

📆  Epoch 26/164

📊  Epoch 26 — Loss: 0.5601 — Acc: 0.9794

📈  Val — Loss: 0.6999 — Acc: 0.7890

💾  Saved new best model — Val Acc: 0.7890

📆  Epoch 27/164

📊  Epoch 27 — Loss: 0.6069 — Acc: 0.9767

📈  Val — Loss: 0.7900 — Acc: 0.7588

📆  Epoch 28/164

📊  Epoch 28 — Loss: 0.5676 — Acc: 0.9803

📈  Val — Loss: 0.7084 — Acc: 0.7806

📆  Epoch 29/164

📊  Epoch 29 — Loss: 0.5888 — Acc: 0.9798

📈  Val — Loss: 0.7641 — Acc: 0.7730

📆  Epoch 30/164

📊  Epoch 30 — Loss: 0.5826 — Acc: 0.9826

📈  Val — Loss: 0.7032 — Acc: 0.7920

💾  Saved new best model — Val Acc: 0.7920

📆  Epoch 31/164

📊  Epoch 31 — Loss: 0.5445 — Acc: 0.9845

📈  Val — Loss: 0.7473 — Acc: 0.7768

📆  Epoch 32/164

📊  Epoch 32 — Loss: 0.5715 — Acc: 0.9831

📈  Val — Loss: 0.7762 — Acc: 0.7700

📆  Epoch 33/164

📊  Epoch 33 — Loss: 0.5887 — Acc: 0.9825

📈  Val — Loss: 0.7958 — Acc: 0.7702

📆  Epoch 34/164

📊  Epoch 34 — Loss: 0.5586 — Acc: 0.9858

📈  Val — Loss: 0.7684 — Acc: 0.7750

📆  Epoch 35/164

📊  Epoch 35 — Loss: 0.5671 — Acc: 0.9871

📈  Val — Loss: 0.6923 — Acc: 0.7926

💾  Saved new best model — Val Acc: 0.7926

📆  Epoch 36/164

📊  Epoch 36 — Loss: 0.5730 — Acc: 0.9862

📈  Val — Loss: 0.6831 — Acc: 0.7928

💾  Saved new best model — Val Acc: 0.7928

📆  Epoch 37/164

📊  Epoch 37 — Loss: 0.5611 — Acc: 0.9893

📈  Val — Loss: 0.7338 — Acc: 0.7880

📆  Epoch 38/164

📊  Epoch 38 — Loss: 0.5507 — Acc: 0.9890

📈  Val — Loss: 0.7510 — Acc: 0.7746

📆  Epoch 39/164

📊  Epoch 39 — Loss: 0.5720 — Acc: 0.9884

📈  Val — Loss: 0.7098 — Acc: 0.7928

📆  Epoch 40/164

📊  Epoch 40 — Loss: 0.5652 — Acc: 0.9874

📈  Val — Loss: 0.6861 — Acc: 0.7952

💾  Saved new best model — Val Acc: 0.7952

📆  Epoch 41/164

📊  Epoch 41 — Loss: 0.5876 — Acc: 0.9875

📈  Val — Loss: 0.7459 — Acc: 0.7808

📆  Epoch 42/164

📊  Epoch 42 — Loss: 0.5626 — Acc: 0.9893

📈  Val — Loss: 0.6875 — Acc: 0.7954

💾  Saved new best model — Val Acc: 0.7954

📆  Epoch 43/164

📊  Epoch 43 — Loss: 0.5504 — Acc: 0.9904

📈  Val — Loss: 0.6706 — Acc: 0.8012

💾  Saved new best model — Val Acc: 0.8012

📆  Epoch 44/164

📊  Epoch 44 — Loss: 0.5522 — Acc: 0.9902

📈  Val — Loss: 0.7385 — Acc: 0.7910

📆  Epoch 45/164

📊  Epoch 45 — Loss: 0.5500 — Acc: 0.9913

📈  Val — Loss: 0.7256 — Acc: 0.7910

📆  Epoch 46/164

📊  Epoch 46 — Loss: 0.5424 — Acc: 0.9901

📈  Val — Loss: 0.7089 — Acc: 0.7878

📆  Epoch 47/164

📊  Epoch 47 — Loss: 0.5407 — Acc: 0.9903

📈  Val — Loss: 0.7477 — Acc: 0.7830

📆  Epoch 48/164

📊  Epoch 48 — Loss: 0.5491 — Acc: 0.9908

📈  Val — Loss: 0.7107 — Acc: 0.7940

📆  Epoch 49/164

📊  Epoch 49 — Loss: 0.5375 — Acc: 0.9919

📈  Val — Loss: 0.7066 — Acc: 0.7954

📆  Epoch 50/164

📊  Epoch 50 — Loss: 0.5337 — Acc: 0.9918

📈  Val — Loss: 0.7572 — Acc: 0.7818

📆  Epoch 51/164

📊  Epoch 51 — Loss: 0.5460 — Acc: 0.9912

📈  Val — Loss: 0.7343 — Acc: 0.7830

📆  Epoch 52/164

📊  Epoch 52 — Loss: 0.5194 — Acc: 0.9925

📈  Val — Loss: 0.6777 — Acc: 0.7978

📆  Epoch 53/164

📊  Epoch 53 — Loss: 0.5319 — Acc: 0.9918

📈  Val — Loss: 0.7380 — Acc: 0.7816

📆  Epoch 54/164

📊  Epoch 54 — Loss: 0.5403 — Acc: 0.9921

📈  Val — Loss: 0.6598 — Acc: 0.8114

💾  Saved new best model — Val Acc: 0.8114

📆  Epoch 55/164

📊  Epoch 55 — Loss: 0.5184 — Acc: 0.9922

📈  Val — Loss: 0.6528 — Acc: 0.8096

📆  Epoch 56/164

📊  Epoch 56 — Loss: 0.5398 — Acc: 0.9920

📈  Val — Loss: 0.7088 — Acc: 0.7976

📆  Epoch 57/164

📊  Epoch 57 — Loss: 0.5106 — Acc: 0.9932

📈  Val — Loss: 0.7287 — Acc: 0.7860

📆  Epoch 58/164

📊  Epoch 58 — Loss: 0.5343 — Acc: 0.9920

📈  Val — Loss: 0.7001 — Acc: 0.8000

📆  Epoch 59/164

📊  Epoch 59 — Loss: 0.5503 — Acc: 0.9925

📈  Val — Loss: 0.7213 — Acc: 0.7876

📆  Epoch 60/164

📊  Epoch 60 — Loss: 0.5072 — Acc: 0.9930

📈  Val — Loss: 0.7027 — Acc: 0.7946

📆  Epoch 61/164

📊  Epoch 61 — Loss: 0.5406 — Acc: 0.9912

📈  Val — Loss: 0.7611 — Acc: 0.7788

📆  Epoch 62/164

📊  Epoch 62 — Loss: 0.5189 — Acc: 0.9933

📈  Val — Loss: 0.7105 — Acc: 0.7946

📆  Epoch 63/164

📊  Epoch 63 — Loss: 0.5139 — Acc: 0.9928

📈  Val — Loss: 0.7005 — Acc: 0.7924

📆  Epoch 64/164

📊  Epoch 64 — Loss: 0.5283 — Acc: 0.9926

📈  Val — Loss: 0.7426 — Acc: 0.7846

📆  Epoch 65/164

📊  Epoch 65 — Loss: 0.5249 — Acc: 0.9930

📈  Val — Loss: 0.8215 — Acc: 0.7684

📆  Epoch 66/164

📊  Epoch 66 — Loss: 0.5279 — Acc: 0.9934

📈  Val — Loss: 0.7040 — Acc: 0.7924

📆  Epoch 67/164

📊  Epoch 67 — Loss: 0.5335 — Acc: 0.9933

📈  Val — Loss: 0.7183 — Acc: 0.7844

📆  Epoch 68/164

📊  Epoch 68 — Loss: 0.5451 — Acc: 0.9933

📈  Val — Loss: 0.7095 — Acc: 0.7982

📆  Epoch 69/164

📊  Epoch 69 — Loss: 0.5281 — Acc: 0.9926

📈  Val — Loss: 0.7244 — Acc: 0.7922

📆  Epoch 70/164

📊  Epoch 70 — Loss: 0.5276 — Acc: 0.9927

📈  Val — Loss: 0.6952 — Acc: 0.7934

📆  Epoch 71/164

📊  Epoch 71 — Loss: 0.5494 — Acc: 0.9926

📈  Val — Loss: 0.6551 — Acc: 0.8034

📆  Epoch 72/164

📊  Epoch 72 — Loss: 0.5155 — Acc: 0.9938

📈  Val — Loss: 0.7233 — Acc: 0.7986

📆  Epoch 73/164

📊  Epoch 73 — Loss: 0.5058 — Acc: 0.9938

📈  Val — Loss: 0.7087 — Acc: 0.7926

📆  Epoch 74/164

📊  Epoch 74 — Loss: 0.5014 — Acc: 0.9945

📈  Val — Loss: 0.7272 — Acc: 0.7822

📆  Epoch 75/164

📊  Epoch 75 — Loss: 0.5119 — Acc: 0.9934

📈  Val — Loss: 0.6926 — Acc: 0.8032

📆  Epoch 76/164

📊  Epoch 76 — Loss: 0.5388 — Acc: 0.9928

📈  Val — Loss: 0.7061 — Acc: 0.8000

📆  Epoch 77/164

📊  Epoch 77 — Loss: 0.5299 — Acc: 0.9923

📈  Val — Loss: 0.7016 — Acc: 0.8034

📆  Epoch 78/164

📊  Epoch 78 — Loss: 0.5065 — Acc: 0.9929

📈  Val — Loss: 0.7027 — Acc: 0.7948

📆  Epoch 79/164

📊  Epoch 79 — Loss: 0.5263 — Acc: 0.9938

📈  Val — Loss: 0.7363 — Acc: 0.7918

📆  Epoch 80/164

📊  Epoch 80 — Loss: 0.5076 — Acc: 0.9944

📈  Val — Loss: 0.7188 — Acc: 0.7928

📆  Epoch 81/164

📊  Epoch 81 — Loss: 0.4760 — Acc: 0.9942

📈  Val — Loss: 0.7031 — Acc: 0.7870

📆  Epoch 82/164

📊  Epoch 82 — Loss: 0.5108 — Acc: 0.9948

📈  Val — Loss: 0.7225 — Acc: 0.7902

📆  Epoch 83/164

📊  Epoch 83 — Loss: 0.5535 — Acc: 0.9930

📈  Val — Loss: 0.7286 — Acc: 0.7906

📆  Epoch 84/164

📊  Epoch 84 — Loss: 0.5494 — Acc: 0.9919

📈  Val — Loss: 0.6965 — Acc: 0.8000

📆  Epoch 85/164

📊  Epoch 85 — Loss: 0.5290 — Acc: 0.9934

📈  Val — Loss: 0.7380 — Acc: 0.7892

📆  Epoch 86/164

📊  Epoch 86 — Loss: 0.5192 — Acc: 0.9934

📈  Val — Loss: 0.6908 — Acc: 0.7964

📆  Epoch 87/164

📊  Epoch 87 — Loss: 0.5160 — Acc: 0.9940

📈  Val — Loss: 0.6818 — Acc: 0.7996

📆  Epoch 88/164

📊  Epoch 88 — Loss: 0.5068 — Acc: 0.9947

📈  Val — Loss: 0.7068 — Acc: 0.7960

📆  Epoch 89/164

📊  Epoch 89 — Loss: 0.5200 — Acc: 0.9936

📈  Val — Loss: 0.7124 — Acc: 0.7962

📆  Epoch 90/164

📊  Epoch 90 — Loss: 0.5166 — Acc: 0.9936

📈  Val — Loss: 0.6503 — Acc: 0.8130

💾  Saved new best model — Val Acc: 0.8130

📆  Epoch 91/164

📊  Epoch 91 — Loss: 0.5438 — Acc: 0.9947

📈  Val — Loss: 0.6875 — Acc: 0.8056

📆  Epoch 92/164

📊  Epoch 92 — Loss: 0.5165 — Acc: 0.9951

📈  Val — Loss: 0.6830 — Acc: 0.8040

📆  Epoch 93/164

📊  Epoch 93 — Loss: 0.5384 — Acc: 0.9938

📈  Val — Loss: 0.7515 — Acc: 0.7884

📆  Epoch 94/164

📊  Epoch 94 — Loss: 0.5118 — Acc: 0.9942

📈  Val — Loss: 0.6853 — Acc: 0.8008

📆  Epoch 95/164

📊  Epoch 95 — Loss: 0.5108 — Acc: 0.9942

📈  Val — Loss: 0.7146 — Acc: 0.7946

📆  Epoch 96/164

📊  Epoch 96 — Loss: 0.5092 — Acc: 0.9944

📈  Val — Loss: 0.6616 — Acc: 0.8112

📆  Epoch 97/164

📊  Epoch 97 — Loss: 0.5380 — Acc: 0.9934

📈  Val — Loss: 0.7333 — Acc: 0.7848

📆  Epoch 98/164

📊  Epoch 98 — Loss: 0.5136 — Acc: 0.9948

📈  Val — Loss: 0.7314 — Acc: 0.7992

📆  Epoch 99/164

📊  Epoch 99 — Loss: 0.5192 — Acc: 0.9949

📈  Val — Loss: 0.7568 — Acc: 0.7844

📆  Epoch 100/164

📊  Epoch 100 — Loss: 0.5012 — Acc: 0.9955

📈  Val — Loss: 0.6883 — Acc: 0.8020

📆  Epoch 101/164

📊  Epoch 101 — Loss: 0.5044 — Acc: 0.9944

📈  Val — Loss: 0.7123 — Acc: 0.7958

📆  Epoch 102/164

📊  Epoch 102 — Loss: 0.4964 — Acc: 0.9944

📈  Val — Loss: 0.7286 — Acc: 0.7914

📆  Epoch 103/164

📊  Epoch 103 — Loss: 0.5094 — Acc: 0.9947

📈  Val — Loss: 0.6947 — Acc: 0.7954

📆  Epoch 104/164

📊  Epoch 104 — Loss: 0.4988 — Acc: 0.9952

📈  Val — Loss: 0.6886 — Acc: 0.8038

📆  Epoch 105/164

📊  Epoch 105 — Loss: 0.5060 — Acc: 0.9941

📈  Val — Loss: 0.6950 — Acc: 0.8034

📆  Epoch 106/164

📊  Epoch 106 — Loss: 0.5140 — Acc: 0.9939

📈  Val — Loss: 0.7392 — Acc: 0.7890

📆  Epoch 107/164

📊  Epoch 107 — Loss: 0.4971 — Acc: 0.9944

📈  Val — Loss: 0.7431 — Acc: 0.7830

📆  Epoch 108/164

📊  Epoch 108 — Loss: 0.4901 — Acc: 0.9955

📈  Val — Loss: 0.6859 — Acc: 0.7956

📆  Epoch 109/164

📊  Epoch 109 — Loss: 0.4972 — Acc: 0.9950

📈  Val — Loss: 0.7054 — Acc: 0.7950

📆  Epoch 110/164

📊  Epoch 110 — Loss: 0.5153 — Acc: 0.9938

📈  Val — Loss: 0.6863 — Acc: 0.8032

📆  Epoch 111/164

📊  Epoch 111 — Loss: 0.5037 — Acc: 0.9946

📈  Val — Loss: 0.7104 — Acc: 0.8006

📆  Epoch 112/164

📊  Epoch 112 — Loss: 0.4843 — Acc: 0.9953

📈  Val — Loss: 0.6882 — Acc: 0.7988

📆  Epoch 113/164

📊  Epoch 113 — Loss: 0.4967 — Acc: 0.9948

📈  Val — Loss: 0.7149 — Acc: 0.7942

📆  Epoch 114/164

📊  Epoch 114 — Loss: 0.5052 — Acc: 0.9942

📈  Val — Loss: 0.7293 — Acc: 0.7938

📆  Epoch 115/164

📊  Epoch 115 — Loss: 0.5188 — Acc: 0.9944

📈  Val — Loss: 0.6927 — Acc: 0.8032

📆  Epoch 116/164

📊  Epoch 116 — Loss: 0.5361 — Acc: 0.9937

📈  Val — Loss: 0.6784 — Acc: 0.8106

📆  Epoch 117/164

📊  Epoch 117 — Loss: 0.5138 — Acc: 0.9948

📈  Val — Loss: 0.6962 — Acc: 0.7958

📆  Epoch 118/164

📊  Epoch 118 — Loss: 0.5198 — Acc: 0.9948

📈  Val — Loss: 0.7048 — Acc: 0.8010

📆  Epoch 119/164

📊  Epoch 119 — Loss: 0.5166 — Acc: 0.9947

📈  Val — Loss: 0.7180 — Acc: 0.7932

📆  Epoch 120/164

📊  Epoch 120 — Loss: 0.5185 — Acc: 0.9951

📈  Val — Loss: 0.7146 — Acc: 0.7992

📆  Epoch 121/164

📊  Epoch 121 — Loss: 0.5178 — Acc: 0.9942

📈  Val — Loss: 0.7085 — Acc: 0.8002

📆  Epoch 122/164

📊  Epoch 122 — Loss: 0.5232 — Acc: 0.9941

📈  Val — Loss: 0.7239 — Acc: 0.7890

📆  Epoch 123/164

📊  Epoch 123 — Loss: 0.4947 — Acc: 0.9953

📈  Val — Loss: 0.7055 — Acc: 0.7996

📆  Epoch 124/164

📊  Epoch 124 — Loss: 0.5244 — Acc: 0.9939

📈  Val — Loss: 0.7071 — Acc: 0.7960

📆  Epoch 125/164

📊  Epoch 125 — Loss: 0.4724 — Acc: 0.9965

📈  Val — Loss: 0.7485 — Acc: 0.7852

📆  Epoch 126/164

📊  Epoch 126 — Loss: 0.5286 — Acc: 0.9936

📈  Val — Loss: 0.6761 — Acc: 0.8024

📆  Epoch 127/164

📊  Epoch 127 — Loss: 0.5122 — Acc: 0.9951

📈  Val — Loss: 0.7398 — Acc: 0.7850

📆  Epoch 128/164

📊  Epoch 128 — Loss: 0.5299 — Acc: 0.9940

📈  Val — Loss: 0.7359 — Acc: 0.7882

📆  Epoch 129/164

📊  Epoch 129 — Loss: 0.5182 — Acc: 0.9944

📈  Val — Loss: 0.7120 — Acc: 0.7948

📆  Epoch 130/164

📊  Epoch 130 — Loss: 0.4917 — Acc: 0.9943

📈  Val — Loss: 0.6546 — Acc: 0.8064

📆  Epoch 131/164

📊  Epoch 131 — Loss: 0.5009 — Acc: 0.9954

📈  Val — Loss: 0.7111 — Acc: 0.7950

📆  Epoch 132/164

📊  Epoch 132 — Loss: 0.5041 — Acc: 0.9945

📈  Val — Loss: 0.6808 — Acc: 0.8108

📆  Epoch 133/164

📊  Epoch 133 — Loss: 0.5288 — Acc: 0.9939

📈  Val — Loss: 0.6608 — Acc: 0.8084

📆  Epoch 134/164

📊  Epoch 134 — Loss: 0.4488 — Acc: 0.9959

📈  Val — Loss: 0.6816 — Acc: 0.8054

📆  Epoch 135/164

📊  Epoch 135 — Loss: 0.5083 — Acc: 0.9954

📈  Val — Loss: 0.6758 — Acc: 0.8034

📆  Epoch 136/164

📊  Epoch 136 — Loss: 0.5201 — Acc: 0.9944

📈  Val — Loss: 0.7735 — Acc: 0.7800

📆  Epoch 137/164

📊  Epoch 137 — Loss: 0.5028 — Acc: 0.9954

📈  Val — Loss: 0.6881 — Acc: 0.8008

📆  Epoch 138/164

📊  Epoch 138 — Loss: 0.4933 — Acc: 0.9952

📈  Val — Loss: 0.7345 — Acc: 0.7912

📆  Epoch 139/164

📊  Epoch 139 — Loss: 0.5120 — Acc: 0.9941

📈  Val — Loss: 0.7020 — Acc: 0.7980

📆  Epoch 140/164

📊  Epoch 140 — Loss: 0.4911 — Acc: 0.9955

📈  Val — Loss: 0.6986 — Acc: 0.8012

📆  Epoch 141/164

📊  Epoch 141 — Loss: 0.5177 — Acc: 0.9945

📈  Val — Loss: 0.6911 — Acc: 0.8004

📆  Epoch 142/164

📊  Epoch 142 — Loss: 0.5116 — Acc: 0.9952

📈  Val — Loss: 0.7142 — Acc: 0.7998

📆  Epoch 143/164

📊  Epoch 143 — Loss: 0.5068 — Acc: 0.9955

📈  Val — Loss: 0.6895 — Acc: 0.8024

📆  Epoch 144/164

📊  Epoch 144 — Loss: 0.5045 — Acc: 0.9948

📈  Val — Loss: 0.7040 — Acc: 0.8006

📆  Epoch 145/164

📊  Epoch 145 — Loss: 0.4996 — Acc: 0.9951

📈  Val — Loss: 0.7069 — Acc: 0.7982

📆  Epoch 146/164

📊  Epoch 146 — Loss: 0.5127 — Acc: 0.9948

📈  Val — Loss: 0.7054 — Acc: 0.7956

📆  Epoch 147/164

📊  Epoch 147 — Loss: 0.5105 — Acc: 0.9960

📈  Val — Loss: 0.7036 — Acc: 0.8016

📆  Epoch 148/164

📊  Epoch 148 — Loss: 0.5049 — Acc: 0.9948

📈  Val — Loss: 0.6913 — Acc: 0.8020

📆  Epoch 149/164

📊  Epoch 149 — Loss: 0.5080 — Acc: 0.9942

📈  Val — Loss: 0.7019 — Acc: 0.7930

📆  Epoch 150/164

📊  Epoch 150 — Loss: 0.4970 — Acc: 0.9962

📈  Val — Loss: 0.6828 — Acc: 0.8056

📆  Epoch 151/164

📊  Epoch 151 — Loss: 0.5135 — Acc: 0.9948

📈  Val — Loss: 0.6826 — Acc: 0.8066

📆  Epoch 152/164

📊  Epoch 152 — Loss: 0.5214 — Acc: 0.9940

📈  Val — Loss: 0.6961 — Acc: 0.8024

📆  Epoch 153/164

📊  Epoch 153 — Loss: 0.4957 — Acc: 0.9957

📈  Val — Loss: 0.6791 — Acc: 0.8082

📆  Epoch 154/164

📊  Epoch 154 — Loss: 0.4984 — Acc: 0.9956

📈  Val — Loss: 0.7003 — Acc: 0.8012

📆  Epoch 155/164

📊  Epoch 155 — Loss: 0.5366 — Acc: 0.9944

📈  Val — Loss: 0.7109 — Acc: 0.7984

📆  Epoch 156/164

📊  Epoch 156 — Loss: 0.5093 — Acc: 0.9952

📈  Val — Loss: 0.6849 — Acc: 0.8042

📆  Epoch 157/164

📊  Epoch 157 — Loss: 0.4984 — Acc: 0.9954

📈  Val — Loss: 0.6880 — Acc: 0.8046

📆  Epoch 158/164

📊  Epoch 158 — Loss: 0.5040 — Acc: 0.9955

📈  Val — Loss: 0.7012 — Acc: 0.8046

📆  Epoch 159/164

📊  Epoch 159 — Loss: 0.5170 — Acc: 0.9953

📈  Val — Loss: 0.7406 — Acc: 0.7966

📆  Epoch 160/164

📊  Epoch 160 — Loss: 0.5160 — Acc: 0.9951

📈  Val — Loss: 0.7015 — Acc: 0.8030

📆  Epoch 161/164

📊  Epoch 161 — Loss: 0.5078 — Acc: 0.9956

📈  Val — Loss: 0.7038 — Acc: 0.7976

📆  Epoch 162/164

📊  Epoch 162 — Loss: 0.4901 — Acc: 0.9955

📈  Val — Loss: 0.6919 — Acc: 0.8068

📆  Epoch 163/164

📊  Epoch 163 — Loss: 0.4740 — Acc: 0.9961

📈  Val — Loss: 0.7157 — Acc: 0.7924

📆  Epoch 164/164

📊  Epoch 164 — Loss: 0.5276 — Acc: 0.9957

📈  Val — Loss: 0.7179 — Acc: 0.7978

🎯  _save_training_history

⚠️   Failing to save history:
Object of type float32 is not JSON serializable

🎯  extract_history_metrics

📥  Restored best model from:
/content/drive/MyDrive/src/cifar-susume/artifact/checkpoint/m9_r1_m9_res_full/best.keras

🎯  evaluate_model

🎯  extract_history_metrics

🎯  _create_evaluation_dictionary

📊  Dumping experiment results:
[
  {
    "model": 9,
    "run": 1,
    "config_name": "m9_res_full",
    "date": "2025-05-26",
    "time": "15:10:35",
    "duration": "0:43:52",
    "parameters": {
      "LIGHT_MODE": false,
      "AUGMENT_MODE": {
        "enabled": false,
        "random_crop": false,
        "random_flip": false,
        "cutout": false
      },
      "L2_MODE": {
        "enabled": true,
        "lambda": 0.0005
      },
      "DROPOUT_MODE": {
        "enabled": false,
        "rate": 0.0
      },
      "OPTIMIZER": {
        "type": "sgd",
        "learning_rate": 0.05,
        "momentum": 0.9
      },
      "SCHEDULE_MODE": {
        "enabled": true,
        "warmup_epochs": 5,
        "factor": null,
        "patience": null,
        "min_lr": null
      },
      "EARLY_STOP_MODE": {
        "enabled": false,
        "patience": null,
        "restore_best_weights": null
      },
      "AVERAGE_MODE": {
        "enabled": true,
        "start_epoch": 150
      },
      "TTA_MODE": {
        "enabled": false,
        "runs": 1
      },
      "MIXUP_MODE": {
        "enabled": true,
        "alpha": 0.2
      },
      "EPOCHS_COUNT": 164,
      "BATCH_SIZE": 128
    },
    "min_train_loss": 0.44883596897125244,
    "min_train_loss_epoch": 134,
    "max_train_acc": 0.9964500069618225,
    "max_train_acc_epoch": 125,
    "min_val_loss": 0.6502788066864014,
    "min_val_loss_epoch": 90,
    "max_val_acc": 0.8130000233650208,
    "max_val_acc_epoch": 90,
    "final_test_loss": 2.7552106380462646,
    "final_test_acc": 0.7940000295639038
  }
]

✅   m9 run 1 with 'm9_res_full' successfully executed

📦   Completed 1 total experiment runs
