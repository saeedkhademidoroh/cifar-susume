
📜  Logging experiment output:
/content/drive/MyDrive/src/cifar-susume/artifact/log/log_2025-05-27_06-19-00.txt

🎯  _load_previous_results

⚙️   Piplining experiment 1/1

🎯  _run_single_pipeline_entry

🎯  load_config

📂  Loading configuration file:
/content/drive/MyDrive/src/cifar-susume/artifact/config/m9_base_res.json

🎯  _ensure_output_directories

📂  Ensuring output directories
/content/drive/MyDrive/src/cifar-susume/artifact/log
/content/drive/MyDrive/src/cifar-susume/artifact/checkpoint
/content/drive/MyDrive/src/cifar-susume/artifact/result
/content/drive/MyDrive/src/cifar-susume/artifact/model
/content/drive/MyDrive/src/cifar-susume/artifact/error

🚀  Launching experiment m9_r1 with 'm9_base_res'

🎯  build_dataset

🎯  build_augmentation_transform

🎯  build_normalization_transform

🎯  build_normalization_transform

🎯  build_model

Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input_layer         │ (None, 32, 32, 3) │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d (Conv2D)     │ (None, 32, 32,    │        448 │ input_layer[0][0] │
│                     │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalization │ (None, 32, 32,    │         64 │ conv2d[0][0]      │
│ (BatchNormalizatio… │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation          │ (None, 32, 32,    │          0 │ batch_normalizat… │
│ (Activation)        │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_1 (Conv2D)   │ (None, 32, 32,    │      2,320 │ activation[0][0]  │
│                     │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32,    │         64 │ conv2d_1[0][0]    │
│ (BatchNormalizatio… │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_1        │ (None, 32, 32,    │          0 │ batch_normalizat… │
│ (Activation)        │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_2 (Conv2D)   │ (None, 32, 32,    │      2,320 │ activation_1[0][… │
│                     │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32,    │         64 │ conv2d_2[0][0]    │
│ (BatchNormalizatio… │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add (Add)           │ (None, 32, 32,    │          0 │ batch_normalizat… │
│                     │ 16)               │            │ activation[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_2        │ (None, 32, 32,    │          0 │ add[0][0]         │
│ (Activation)        │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_3 (Conv2D)   │ (None, 32, 32,    │      2,320 │ activation_2[0][… │
│                     │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32,    │         64 │ conv2d_3[0][0]    │
│ (BatchNormalizatio… │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_3        │ (None, 32, 32,    │          0 │ batch_normalizat… │
│ (Activation)        │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_4 (Conv2D)   │ (None, 32, 32,    │      2,320 │ activation_3[0][… │
│                     │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32,    │         64 │ conv2d_4[0][0]    │
│ (BatchNormalizatio… │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_1 (Add)         │ (None, 32, 32,    │          0 │ batch_normalizat… │
│                     │ 16)               │            │ activation_2[0][… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_4        │ (None, 32, 32,    │          0 │ add_1[0][0]       │
│ (Activation)        │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_5 (Conv2D)   │ (None, 32, 32,    │      2,320 │ activation_4[0][… │
│                     │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32,    │         64 │ conv2d_5[0][0]    │
│ (BatchNormalizatio… │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_5        │ (None, 32, 32,    │          0 │ batch_normalizat… │
│ (Activation)        │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_6 (Conv2D)   │ (None, 32, 32,    │      2,320 │ activation_5[0][… │
│                     │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32,    │         64 │ conv2d_6[0][0]    │
│ (BatchNormalizatio… │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_2 (Add)         │ (None, 32, 32,    │          0 │ batch_normalizat… │
│                     │ 16)               │            │ activation_4[0][… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_6        │ (None, 32, 32,    │          0 │ add_2[0][0]       │
│ (Activation)        │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_7 (Conv2D)   │ (None, 16, 16,    │      4,640 │ activation_6[0][… │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 16, 16,    │        128 │ conv2d_7[0][0]    │
│ (BatchNormalizatio… │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_7        │ (None, 16, 16,    │          0 │ batch_normalizat… │
│ (Activation)        │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_8 (Conv2D)   │ (None, 16, 16,    │      9,248 │ activation_7[0][… │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 16, 16,    │        128 │ conv2d_8[0][0]    │
│ (BatchNormalizatio… │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_9 (Conv2D)   │ (None, 16, 16,    │        544 │ activation_6[0][… │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_3 (Add)         │ (None, 16, 16,    │          0 │ batch_normalizat… │
│                     │ 32)               │            │ conv2d_9[0][0]    │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_8        │ (None, 16, 16,    │          0 │ add_3[0][0]       │
│ (Activation)        │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_10 (Conv2D)  │ (None, 16, 16,    │      9,248 │ activation_8[0][… │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 16, 16,    │        128 │ conv2d_10[0][0]   │
│ (BatchNormalizatio… │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_9        │ (None, 16, 16,    │          0 │ batch_normalizat… │
│ (Activation)        │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_11 (Conv2D)  │ (None, 16, 16,    │      9,248 │ activation_9[0][… │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 16, 16,    │        128 │ conv2d_11[0][0]   │
│ (BatchNormalizatio… │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_4 (Add)         │ (None, 16, 16,    │          0 │ batch_normalizat… │
│                     │ 32)               │            │ activation_8[0][… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_10       │ (None, 16, 16,    │          0 │ add_4[0][0]       │
│ (Activation)        │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_12 (Conv2D)  │ (None, 16, 16,    │      9,248 │ activation_10[0]… │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 16, 16,    │        128 │ conv2d_12[0][0]   │
│ (BatchNormalizatio… │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_11       │ (None, 16, 16,    │          0 │ batch_normalizat… │
│ (Activation)        │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_13 (Conv2D)  │ (None, 16, 16,    │      9,248 │ activation_11[0]… │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 16, 16,    │        128 │ conv2d_13[0][0]   │
│ (BatchNormalizatio… │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_5 (Add)         │ (None, 16, 16,    │          0 │ batch_normalizat… │
│                     │ 32)               │            │ activation_10[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_12       │ (None, 16, 16,    │          0 │ add_5[0][0]       │
│ (Activation)        │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_14 (Conv2D)  │ (None, 8, 8, 64)  │     18,496 │ activation_12[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 8, 8, 64)  │        256 │ conv2d_14[0][0]   │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_13       │ (None, 8, 8, 64)  │          0 │ batch_normalizat… │
│ (Activation)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_15 (Conv2D)  │ (None, 8, 8, 64)  │     36,928 │ activation_13[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 8, 8, 64)  │        256 │ conv2d_15[0][0]   │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_16 (Conv2D)  │ (None, 8, 8, 64)  │      2,112 │ activation_12[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_6 (Add)         │ (None, 8, 8, 64)  │          0 │ batch_normalizat… │
│                     │                   │            │ conv2d_16[0][0]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_14       │ (None, 8, 8, 64)  │          0 │ add_6[0][0]       │
│ (Activation)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_17 (Conv2D)  │ (None, 8, 8, 64)  │     36,928 │ activation_14[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 8, 8, 64)  │        256 │ conv2d_17[0][0]   │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_15       │ (None, 8, 8, 64)  │          0 │ batch_normalizat… │
│ (Activation)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_18 (Conv2D)  │ (None, 8, 8, 64)  │     36,928 │ activation_15[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 8, 8, 64)  │        256 │ conv2d_18[0][0]   │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_7 (Add)         │ (None, 8, 8, 64)  │          0 │ batch_normalizat… │
│                     │                   │            │ activation_14[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_16       │ (None, 8, 8, 64)  │          0 │ add_7[0][0]       │
│ (Activation)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_19 (Conv2D)  │ (None, 8, 8, 64)  │     36,928 │ activation_16[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 8, 8, 64)  │        256 │ conv2d_19[0][0]   │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_17       │ (None, 8, 8, 64)  │          0 │ batch_normalizat… │
│ (Activation)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_20 (Conv2D)  │ (None, 8, 8, 64)  │     36,928 │ activation_17[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 8, 8, 64)  │        256 │ conv2d_20[0][0]   │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ add_8 (Add)         │ (None, 8, 8, 64)  │          0 │ batch_normalizat… │
│                     │                   │            │ activation_16[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_18       │ (None, 8, 8, 64)  │          0 │ add_8[0][0]       │
│ (Activation)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ global_average_poo… │ (None, 64)        │          0 │ activation_18[0]… │
│ (GlobalAveragePool… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense (Dense)       │ (None, 10)        │        650 │ global_average_p… │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 274,442 (1.05 MB)
 Trainable params: 273,066 (1.04 MB)
 Non-trainable params: 1,376 (5.38 KB)

🎯  train_model

🎯  _resume_from_checkpoint

🎯  _load_from_checkpoint

🎯  _split_dataset

🎯  _prepare_checkpoint_callback

🎯  __init__ (RecoveryCheckpoint)


🎯  _print_training_context

🖥️   Available compute devices:
  • /device:CPU:0 (CPU)
  • /device:GPU:0 (GPU)

🧮  GPU detected: True
  • /physical_device:GPU:0

🧠  Printing training configuration:
Light Mode:         OFF — Using reduced dataset for fast testing
Augmentation:       ON — Random Crop, Horizontal Flip
L2 Regularization:  ON (λ = 0.0001)
Dropout:            OFF (rate = 0.0)
Optimizer:          SGD (lr = 0.1)
Momentum:           0.9
LR Scheduler:       ON — warmup for 0 epochs, decay factor 0.1
Early Stopping:     OFF
Weight Averaging:   OFF
Test-Time Augment:  OFF
MixUp:              OFF
Epochs:             164
Batch Size:         128

📆  Epoch 1/164

📊  Epoch 1 — Loss: 1.8654 — Acc: 0.3067

📈  Val — Loss: 1.9994 — Acc: 0.3274

💾  Saved new best model — Val Acc: 0.3274

📆  Epoch 2/164

📊  Epoch 2 — Loss: 1.5052 — Acc: 0.4447

📈  Val — Loss: 1.7810 — Acc: 0.3934

💾  Saved new best model — Val Acc: 0.3934

📆  Epoch 3/164

📊  Epoch 3 — Loss: 1.2373 — Acc: 0.5530

📈  Val — Loss: 1.2634 — Acc: 0.5472

💾  Saved new best model — Val Acc: 0.5472

📆  Epoch 4/164

📊  Epoch 4 — Loss: 1.0430 — Acc: 0.6271

📈  Val — Loss: 1.8683 — Acc: 0.4402

📆  Epoch 5/164

📊  Epoch 5 — Loss: 0.9049 — Acc: 0.6793

📈  Val — Loss: 1.3780 — Acc: 0.5752

💾  Saved new best model — Val Acc: 0.5752

📆  Epoch 6/164

📊  Epoch 6 — Loss: 0.7875 — Acc: 0.7229

📈  Val — Loss: 1.0185 — Acc: 0.6622

💾  Saved new best model — Val Acc: 0.6622

📆  Epoch 7/164

📊  Epoch 7 — Loss: 0.6982 — Acc: 0.7561

📈  Val — Loss: 1.0546 — Acc: 0.6526

📆  Epoch 8/164

📊  Epoch 8 — Loss: 0.6212 — Acc: 0.7791

📈  Val — Loss: 1.2978 — Acc: 0.6028

📆  Epoch 9/164

📊  Epoch 9 — Loss: 0.5672 — Acc: 0.8000

📈  Val — Loss: 1.3365 — Acc: 0.6052

📆  Epoch 10/164

📊  Epoch 10 — Loss: 0.5053 — Acc: 0.8229

📈  Val — Loss: 1.0828 — Acc: 0.6598

📆  Epoch 11/164

📊  Epoch 11 — Loss: 0.4570 — Acc: 0.8366

📈  Val — Loss: 1.0295 — Acc: 0.6836

💾  Saved new best model — Val Acc: 0.6836

📆  Epoch 12/164

📊  Epoch 12 — Loss: 0.4116 — Acc: 0.8533

📈  Val — Loss: 1.1179 — Acc: 0.6716

📆  Epoch 13/164

📊  Epoch 13 — Loss: 0.3841 — Acc: 0.8637

📈  Val — Loss: 1.0807 — Acc: 0.6898

💾  Saved new best model — Val Acc: 0.6898

📆  Epoch 14/164

📊  Epoch 14 — Loss: 0.3497 — Acc: 0.8748

📈  Val — Loss: 1.1410 — Acc: 0.6992

💾  Saved new best model — Val Acc: 0.6992

📆  Epoch 15/164

📊  Epoch 15 — Loss: 0.3062 — Acc: 0.8902

📈  Val — Loss: 1.3229 — Acc: 0.7062

💾  Saved new best model — Val Acc: 0.7062

📆  Epoch 16/164

📊  Epoch 16 — Loss: 0.2725 — Acc: 0.9021

📈  Val — Loss: 1.7075 — Acc: 0.6298

📆  Epoch 17/164

📊  Epoch 17 — Loss: 0.2653 — Acc: 0.9043

📈  Val — Loss: 1.3511 — Acc: 0.6914

📆  Epoch 18/164

📊  Epoch 18 — Loss: 0.2406 — Acc: 0.9137

📈  Val — Loss: 1.1232 — Acc: 0.7132

💾  Saved new best model — Val Acc: 0.7132

📆  Epoch 19/164

📊  Epoch 19 — Loss: 0.2152 — Acc: 0.9229

📈  Val — Loss: 1.3977 — Acc: 0.6750

📆  Epoch 20/164

📊  Epoch 20 — Loss: 0.1878 — Acc: 0.9317

📈  Val — Loss: 1.8392 — Acc: 0.6416

📆  Epoch 21/164

📊  Epoch 21 — Loss: 0.1864 — Acc: 0.9331

📈  Val — Loss: 1.4751 — Acc: 0.6880

📆  Epoch 22/164

📊  Epoch 22 — Loss: 0.1641 — Acc: 0.9402

📈  Val — Loss: 1.2261 — Acc: 0.7120

📆  Epoch 23/164

📊  Epoch 23 — Loss: 0.1590 — Acc: 0.9435

📈  Val — Loss: 1.6693 — Acc: 0.6804

📆  Epoch 24/164

📊  Epoch 24 — Loss: 0.1435 — Acc: 0.9481

📈  Val — Loss: 1.4562 — Acc: 0.6926

📆  Epoch 25/164

📊  Epoch 25 — Loss: 0.1265 — Acc: 0.9548

📈  Val — Loss: 1.4574 — Acc: 0.7220

💾  Saved new best model — Val Acc: 0.7220

📆  Epoch 26/164

📊  Epoch 26 — Loss: 0.0999 — Acc: 0.9650

📈  Val — Loss: 1.4934 — Acc: 0.7026

📆  Epoch 27/164

📊  Epoch 27 — Loss: 0.1304 — Acc: 0.9530

📈  Val — Loss: 1.5232 — Acc: 0.7332

💾  Saved new best model — Val Acc: 0.7332

📆  Epoch 28/164

📊  Epoch 28 — Loss: 0.1110 — Acc: 0.9612

📈  Val — Loss: 1.2723 — Acc: 0.7442

💾  Saved new best model — Val Acc: 0.7442

📆  Epoch 29/164

📊  Epoch 29 — Loss: 0.0819 — Acc: 0.9704

📈  Val — Loss: 1.3625 — Acc: 0.7344

📆  Epoch 30/164

📊  Epoch 30 — Loss: 0.0858 — Acc: 0.9693

📈  Val — Loss: 1.4745 — Acc: 0.7208

📆  Epoch 31/164

📊  Epoch 31 — Loss: 0.0858 — Acc: 0.9691

📈  Val — Loss: 1.5959 — Acc: 0.7212

📆  Epoch 32/164

📊  Epoch 32 — Loss: 0.0831 — Acc: 0.9700

📈  Val — Loss: 1.4064 — Acc: 0.7298

📆  Epoch 33/164

📊  Epoch 33 — Loss: 0.0691 — Acc: 0.9746

📈  Val — Loss: 1.6906 — Acc: 0.7184

📆  Epoch 34/164

📊  Epoch 34 — Loss: 0.0759 — Acc: 0.9734

📈  Val — Loss: 1.4479 — Acc: 0.7426

📆  Epoch 35/164

📊  Epoch 35 — Loss: 0.0585 — Acc: 0.9794

📈  Val — Loss: 1.4118 — Acc: 0.7532

💾  Saved new best model — Val Acc: 0.7532

📆  Epoch 36/164

📊  Epoch 36 — Loss: 0.0617 — Acc: 0.9786

📈  Val — Loss: 1.5688 — Acc: 0.7416

📆  Epoch 37/164

📊  Epoch 37 — Loss: 0.0626 — Acc: 0.9781

📈  Val — Loss: 1.7481 — Acc: 0.7194

📆  Epoch 38/164

📊  Epoch 38 — Loss: 0.0561 — Acc: 0.9808

📈  Val — Loss: 1.7151 — Acc: 0.7304

📆  Epoch 39/164

📊  Epoch 39 — Loss: 0.0533 — Acc: 0.9813

📈  Val — Loss: 1.5505 — Acc: 0.7376

📆  Epoch 40/164

📊  Epoch 40 — Loss: 0.0457 — Acc: 0.9843

📈  Val — Loss: 1.4531 — Acc: 0.7436

📆  Epoch 41/164

📊  Epoch 41 — Loss: 0.0445 — Acc: 0.9848

📈  Val — Loss: 1.5696 — Acc: 0.7378

📆  Epoch 42/164

📊  Epoch 42 — Loss: 0.0394 — Acc: 0.9861

📈  Val — Loss: 1.5639 — Acc: 0.7472

📆  Epoch 43/164

📊  Epoch 43 — Loss: 0.0335 — Acc: 0.9886

📈  Val — Loss: 1.8414 — Acc: 0.7134

📆  Epoch 44/164

📊  Epoch 44 — Loss: 0.0410 — Acc: 0.9853

📈  Val — Loss: 1.8294 — Acc: 0.7416

📆  Epoch 45/164

📊  Epoch 45 — Loss: 0.0446 — Acc: 0.9844

📈  Val — Loss: 1.6509 — Acc: 0.7396

📆  Epoch 46/164

📊  Epoch 46 — Loss: 0.0407 — Acc: 0.9858

📈  Val — Loss: 1.6165 — Acc: 0.7474

📆  Epoch 47/164

📊  Epoch 47 — Loss: 0.0366 — Acc: 0.9866

📈  Val — Loss: 1.9758 — Acc: 0.7196

📆  Epoch 48/164

📊  Epoch 48 — Loss: 0.0404 — Acc: 0.9858

📈  Val — Loss: 1.4698 — Acc: 0.7616

💾  Saved new best model — Val Acc: 0.7616

📆  Epoch 49/164

📊  Epoch 49 — Loss: 0.0377 — Acc: 0.9874

📈  Val — Loss: 1.5055 — Acc: 0.7602

📆  Epoch 50/164

📊  Epoch 50 — Loss: 0.0364 — Acc: 0.9873

📈  Val — Loss: 1.6954 — Acc: 0.7376

📆  Epoch 51/164

📊  Epoch 51 — Loss: 0.0371 — Acc: 0.9871

📈  Val — Loss: 1.9789 — Acc: 0.7080

📆  Epoch 52/164

📊  Epoch 52 — Loss: 0.0388 — Acc: 0.9863

📈  Val — Loss: 1.5172 — Acc: 0.7486

📆  Epoch 53/164

📊  Epoch 53 — Loss: 0.0254 — Acc: 0.9910

📈  Val — Loss: 1.7840 — Acc: 0.7418

📆  Epoch 54/164

📊  Epoch 54 — Loss: 0.0218 — Acc: 0.9928

📈  Val — Loss: 1.5780 — Acc: 0.7588

📆  Epoch 55/164

📊  Epoch 55 — Loss: 0.0181 — Acc: 0.9935

📈  Val — Loss: 1.4100 — Acc: 0.7840

💾  Saved new best model — Val Acc: 0.7840

📆  Epoch 56/164

📊  Epoch 56 — Loss: 0.0141 — Acc: 0.9952

📈  Val — Loss: 1.5406 — Acc: 0.7618

📆  Epoch 57/164

📊  Epoch 57 — Loss: 0.0154 — Acc: 0.9947

📈  Val — Loss: 1.7856 — Acc: 0.7480

📆  Epoch 58/164

📊  Epoch 58 — Loss: 0.0164 — Acc: 0.9945

📈  Val — Loss: 1.7024 — Acc: 0.7556

📆  Epoch 59/164

📊  Epoch 59 — Loss: 0.0208 — Acc: 0.9930

📈  Val — Loss: 1.5583 — Acc: 0.7696

📆  Epoch 60/164

📊  Epoch 60 — Loss: 0.0242 — Acc: 0.9915

📈  Val — Loss: 1.6597 — Acc: 0.7616

📆  Epoch 61/164

📊  Epoch 61 — Loss: 0.0353 — Acc: 0.9880

📈  Val — Loss: 1.8835 — Acc: 0.7396

📆  Epoch 62/164

📊  Epoch 62 — Loss: 0.0339 — Acc: 0.9881

📈  Val — Loss: 1.9146 — Acc: 0.7320

📆  Epoch 63/164

📊  Epoch 63 — Loss: 0.0284 — Acc: 0.9905

📈  Val — Loss: 1.7289 — Acc: 0.7564

📆  Epoch 64/164

📊  Epoch 64 — Loss: 0.0215 — Acc: 0.9926

📈  Val — Loss: 1.6556 — Acc: 0.7576

📆  Epoch 65/164

📊  Epoch 65 — Loss: 0.0163 — Acc: 0.9946

📈  Val — Loss: 1.7683 — Acc: 0.7410

📆  Epoch 66/164

📊  Epoch 66 — Loss: 0.0109 — Acc: 0.9969

📈  Val — Loss: 1.9848 — Acc: 0.7342

📆  Epoch 67/164

📊  Epoch 67 — Loss: 0.0106 — Acc: 0.9968

📈  Val — Loss: 1.7485 — Acc: 0.7608

📆  Epoch 68/164

📊  Epoch 68 — Loss: 0.0074 — Acc: 0.9975

📈  Val — Loss: 1.6770 — Acc: 0.7690

📆  Epoch 69/164

📊  Epoch 69 — Loss: 0.0069 — Acc: 0.9980

📈  Val — Loss: 1.6186 — Acc: 0.7694

📆  Epoch 70/164

📊  Epoch 70 — Loss: 0.0032 — Acc: 0.9992

📈  Val — Loss: 1.5069 — Acc: 0.7802

📆  Epoch 71/164

📊  Epoch 71 — Loss: 0.0020 — Acc: 0.9995

📈  Val — Loss: 1.5044 — Acc: 0.7842

💾  Saved new best model — Val Acc: 0.7842

📆  Epoch 72/164

📊  Epoch 72 — Loss: 0.0016 — Acc: 0.9997

📈  Val — Loss: 1.4987 — Acc: 0.7870

💾  Saved new best model — Val Acc: 0.7870

📆  Epoch 73/164

📊  Epoch 73 — Loss: 0.0008 — Acc: 0.9999

📈  Val — Loss: 1.4999 — Acc: 0.7900

💾  Saved new best model — Val Acc: 0.7900

📆  Epoch 74/164

📊  Epoch 74 — Loss: 0.0006 — Acc: 0.9999

📈  Val — Loss: 1.4997 — Acc: 0.7908

💾  Saved new best model — Val Acc: 0.7908

📆  Epoch 75/164

📊  Epoch 75 — Loss: 0.0006 — Acc: 0.9999

📈  Val — Loss: 1.5194 — Acc: 0.7940

💾  Saved new best model — Val Acc: 0.7940

📆  Epoch 76/164

📊  Epoch 76 — Loss: 0.0004 — Acc: 0.9999

📈  Val — Loss: 1.5150 — Acc: 0.7944

💾  Saved new best model — Val Acc: 0.7944

📆  Epoch 77/164

📊  Epoch 77 — Loss: 0.0003 — Acc: 1.0000

📈  Val — Loss: 1.5110 — Acc: 0.7940

📆  Epoch 78/164

📊  Epoch 78 — Loss: 0.0004 — Acc: 0.9999

📈  Val — Loss: 1.5170 — Acc: 0.7950

💾  Saved new best model — Val Acc: 0.7950

📆  Epoch 79/164

📊  Epoch 79 — Loss: 0.0003 — Acc: 1.0000

📈  Val — Loss: 1.5209 — Acc: 0.7952

💾  Saved new best model — Val Acc: 0.7952

📆  Epoch 80/164

📊  Epoch 80 — Loss: 0.0003 — Acc: 1.0000

📈  Val — Loss: 1.5262 — Acc: 0.7920

📆  Epoch 81/164

📊  Epoch 81 — Loss: 0.0002 — Acc: 1.0000

📈  Val — Loss: 1.5225 — Acc: 0.7958

💾  Saved new best model — Val Acc: 0.7958

📆  Epoch 82/164

📊  Epoch 82 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5303 — Acc: 0.7932

📆  Epoch 83/164

📊  Epoch 83 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5304 — Acc: 0.7944

📆  Epoch 84/164

📊  Epoch 84 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5337 — Acc: 0.7928

📆  Epoch 85/164

📊  Epoch 85 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5414 — Acc: 0.7910

📆  Epoch 86/164

📊  Epoch 86 — Loss: 0.0002 — Acc: 1.0000

📈  Val — Loss: 1.5302 — Acc: 0.7960

💾  Saved new best model — Val Acc: 0.7960

📆  Epoch 87/164

📊  Epoch 87 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5350 — Acc: 0.7952

📆  Epoch 88/164

📊  Epoch 88 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5330 — Acc: 0.7950

📆  Epoch 89/164

📊  Epoch 89 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5414 — Acc: 0.7952

📆  Epoch 90/164

📊  Epoch 90 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5406 — Acc: 0.7944

📆  Epoch 91/164

📊  Epoch 91 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5435 — Acc: 0.7960

📆  Epoch 92/164

📊  Epoch 92 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5509 — Acc: 0.7952

📆  Epoch 93/164

📊  Epoch 93 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5477 — Acc: 0.7960

📆  Epoch 94/164

📊  Epoch 94 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5512 — Acc: 0.7952

📆  Epoch 95/164

📊  Epoch 95 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5517 — Acc: 0.7956

📆  Epoch 96/164

📊  Epoch 96 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5503 — Acc: 0.7950

📆  Epoch 97/164

📊  Epoch 97 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5584 — Acc: 0.7948

📆  Epoch 98/164

📊  Epoch 98 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5550 — Acc: 0.7950

📆  Epoch 99/164

📊  Epoch 99 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5575 — Acc: 0.7932

📆  Epoch 100/164

📊  Epoch 100 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5609 — Acc: 0.7964

💾  Saved new best model — Val Acc: 0.7964

📆  Epoch 101/164

📊  Epoch 101 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5658 — Acc: 0.7950

📆  Epoch 102/164

📊  Epoch 102 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5645 — Acc: 0.7938

📆  Epoch 103/164

📊  Epoch 103 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5624 — Acc: 0.7948

📆  Epoch 104/164

📊  Epoch 104 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5631 — Acc: 0.7932

📆  Epoch 105/164

📊  Epoch 105 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5568 — Acc: 0.7914

📆  Epoch 106/164

📊  Epoch 106 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5734 — Acc: 0.7922

📆  Epoch 107/164

📊  Epoch 107 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5730 — Acc: 0.7940

📆  Epoch 108/164

📊  Epoch 108 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5652 — Acc: 0.7940

📆  Epoch 109/164

📊  Epoch 109 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5659 — Acc: 0.7934

📆  Epoch 110/164

📊  Epoch 110 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5727 — Acc: 0.7936

📆  Epoch 111/164

📊  Epoch 111 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5702 — Acc: 0.7938

📆  Epoch 112/164

📊  Epoch 112 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5701 — Acc: 0.7946

📆  Epoch 113/164

📊  Epoch 113 — Loss: 0.0001 — Acc: 1.0000

📈  Val — Loss: 1.5709 — Acc: 0.7942

📆  Epoch 114/164

📊  Epoch 114 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5730 — Acc: 0.7942

📆  Epoch 115/164

📊  Epoch 115 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5756 — Acc: 0.7948

📆  Epoch 116/164

📊  Epoch 116 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5755 — Acc: 0.7944

📆  Epoch 117/164

📊  Epoch 117 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5762 — Acc: 0.7950

📆  Epoch 118/164

📊  Epoch 118 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5795 — Acc: 0.7946

📆  Epoch 119/164

📊  Epoch 119 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5774 — Acc: 0.7946

📆  Epoch 120/164

📊  Epoch 120 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5758 — Acc: 0.7956

📆  Epoch 121/164

📊  Epoch 121 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5769 — Acc: 0.7944

📆  Epoch 122/164

📊  Epoch 122 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5799 — Acc: 0.7948

📆  Epoch 123/164

📊  Epoch 123 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5807 — Acc: 0.7948

📆  Epoch 124/164

📊  Epoch 124 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5806 — Acc: 0.7952

📆  Epoch 125/164

📊  Epoch 125 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5837 — Acc: 0.7976

💾  Saved new best model — Val Acc: 0.7976

📆  Epoch 126/164

📊  Epoch 126 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5846 — Acc: 0.7964

📆  Epoch 127/164

📊  Epoch 127 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5845 — Acc: 0.7946

📆  Epoch 128/164

📊  Epoch 128 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5846 — Acc: 0.7946

📆  Epoch 129/164

📊  Epoch 129 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5891 — Acc: 0.7952

📆  Epoch 130/164

📊  Epoch 130 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5886 — Acc: 0.7942

📆  Epoch 131/164

📊  Epoch 131 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5892 — Acc: 0.7948

📆  Epoch 132/164

📊  Epoch 132 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5911 — Acc: 0.7950

📆  Epoch 133/164

📊  Epoch 133 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5906 — Acc: 0.7966

📆  Epoch 134/164

📊  Epoch 134 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5920 — Acc: 0.7958

📆  Epoch 135/164

📊  Epoch 135 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5916 — Acc: 0.7964

📆  Epoch 136/164

📊  Epoch 136 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5937 — Acc: 0.7950

📆  Epoch 137/164

📊  Epoch 137 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5938 — Acc: 0.7954

📆  Epoch 138/164

📊  Epoch 138 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5912 — Acc: 0.7950

📆  Epoch 139/164

📊  Epoch 139 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5942 — Acc: 0.7954

📆  Epoch 140/164

📊  Epoch 140 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5900 — Acc: 0.7942

📆  Epoch 141/164

📊  Epoch 141 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5999 — Acc: 0.7946

📆  Epoch 142/164

📊  Epoch 142 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6013 — Acc: 0.7950

📆  Epoch 143/164

📊  Epoch 143 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6004 — Acc: 0.7948

📆  Epoch 144/164

📊  Epoch 144 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6037 — Acc: 0.7942

📆  Epoch 145/164

📊  Epoch 145 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6026 — Acc: 0.7946

📆  Epoch 146/164

📊  Epoch 146 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6036 — Acc: 0.7946

📆  Epoch 147/164

📊  Epoch 147 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6025 — Acc: 0.7948

📆  Epoch 148/164

📊  Epoch 148 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6003 — Acc: 0.7962

📆  Epoch 149/164

📊  Epoch 149 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.5993 — Acc: 0.7960

📆  Epoch 150/164

📊  Epoch 150 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6005 — Acc: 0.7956

📆  Epoch 151/164

📊  Epoch 151 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6009 — Acc: 0.7958

📆  Epoch 152/164

📊  Epoch 152 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6025 — Acc: 0.7958

📆  Epoch 153/164

📊  Epoch 153 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6019 — Acc: 0.7956

📆  Epoch 154/164

📊  Epoch 154 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6039 — Acc: 0.7950

📆  Epoch 155/164

📊  Epoch 155 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6030 — Acc: 0.7946

📆  Epoch 156/164

📊  Epoch 156 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6027 — Acc: 0.7952

📆  Epoch 157/164

📊  Epoch 157 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6042 — Acc: 0.7946

📆  Epoch 158/164

📊  Epoch 158 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6025 — Acc: 0.7952

📆  Epoch 159/164

📊  Epoch 159 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6050 — Acc: 0.7948

📆  Epoch 160/164

📊  Epoch 160 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6061 — Acc: 0.7950

📆  Epoch 161/164

📊  Epoch 161 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6060 — Acc: 0.7958

📆  Epoch 162/164

📊  Epoch 162 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6058 — Acc: 0.7954

📆  Epoch 163/164

📊  Epoch 163 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6070 — Acc: 0.7948

📆  Epoch 164/164

📊  Epoch 164 — Loss: 0.0000 — Acc: 1.0000

📈  Val — Loss: 1.6072 — Acc: 0.7946

🎯  _save_training_history

⚠️   Failing to save history:
Object of type float32 is not JSON serializable

🎯  extract_history_metrics

📥  Restored best model from:
/content/drive/MyDrive/src/cifar-susume/artifact/checkpoint/m9_r1_m9_base_res/best.keras

🎯  evaluate_model

🎯  extract_history_metrics

🎯  _create_evaluation_dictionary

📊  Dumping experiment results:
[
  {
    "model": 9,
    "run": 1,
    "config_name": "m9_base_res",
    "date": "2025-05-27",
    "time": "07:02:39",
    "duration": "0:43:38",
    "parameters": {
      "LIGHT_MODE": false,
      "AUGMENT_MODE": {
        "enabled": true,
        "random_crop": true,
        "random_flip": true,
        "cutout": false
      },
      "L2_MODE": {
        "enabled": true,
        "lambda": 0.0001
      },
      "DROPOUT_MODE": {
        "enabled": false,
        "rate": 0.0
      },
      "OPTIMIZER": {
        "type": "sgd",
        "learning_rate": 0.1,
        "momentum": 0.9
      },
      "SCHEDULE_MODE": {
        "enabled": true,
        "warmup_epochs": 0,
        "factor": null,
        "patience": null,
        "min_lr": null
      },
      "EARLY_STOP_MODE": {
        "enabled": false,
        "patience": 20,
        "restore_best_weights": false
      },
      "AVERAGE_MODE": {
        "enabled": false,
        "start_epoch": 150
      },
      "TTA_MODE": {
        "enabled": false,
        "runs": 1
      },
      "MIXUP_MODE": {
        "enabled": false,
        "alpha": 0.2
      },
      "EPOCHS_COUNT": 164,
      "BATCH_SIZE": 128
    },
    "min_train_loss": 1.7569136616657488e-05,
    "min_train_loss_epoch": 164,
    "max_train_acc": 1.0,
    "max_train_acc_epoch": 81,
    "min_val_loss": 1.0185203552246094,
    "min_val_loss_epoch": 6,
    "max_val_acc": 0.7975999712944031,
    "max_val_acc_epoch": 125,
    "final_test_loss": 3.1867752075195312,
    "final_test_acc": 0.7989000082015991
  }
]

✅   m9 run 1 with 'm9_base_res' successfully executed

📦   Completed 1 total experiment runs
